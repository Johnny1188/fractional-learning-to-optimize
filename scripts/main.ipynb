{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lovely_tensors as lt # can be removed\n",
    "import numpy as np\n",
    "import dill\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from fl2o.optimizee import MLPOptee, CustomParams\n",
    "from fl2o.optimizee_modules import MetaParameter\n",
    "from fl2o.optimizer import GD, Adam, FGD, AFOGD, CFGD, CFGD_ClosedForm, L2O_Update\n",
    "from fl2o.l2o import L2O\n",
    "from fl2o.data import MNIST, CustomTask, generate_least_squares_task\n",
    "from fl2o.training import do_fit, find_best_lr, meta_train, get_optimal_lr\n",
    "from fl2o.utils import plot_log, plotter, plot_metric, apply_publication_plt_settings, plot_strategy, dict_to_str\n",
    "\n",
    "lt.monkey_patch() # can be removed\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "CKPT_PATH = os.getenv(\"CKPT_PATH\")\n",
    "DEVICE = os.getenv(\"DEVICE\", \"cpu\")\n",
    "\n",
    "print(f\"{DATA_PATH=}\\n{CKPT_PATH=}\\n{DEVICE=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load previous checkpoint (and skip meta-training of a new l2O optimizer)\n",
    "ckpt = torch.load(\n",
    "    os.path.join(\n",
    "        CKPT_PATH,\n",
    "        \"l2o\",\n",
    "        \"02-02_21-39__L2O__CFGD_ClosedForm\",\n",
    "        \"ckpt.pth\"\n",
    "        # \"meta_training\",\n",
    "        # \"20.pt\",\n",
    "    ),\n",
    "    map_location=torch.device(DEVICE),\n",
    "    pickle_module=dill,\n",
    ")\n",
    "l2o_dict = ckpt[\"l2o_dict\"]\n",
    "l2o_dict_best = ckpt[\"l2o_dict_best\"]\n",
    "log = ckpt[\"log\"]\n",
    "config = ckpt[\"config\"]\n",
    "print(json.dumps(config, indent=4, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load previous checkpoint (and skip meta-training of a new l2O optimizer)\n",
    "ckpt_2 = torch.load(\n",
    "    os.path.join(\n",
    "        CKPT_PATH,\n",
    "        \"l2o\",\n",
    "        \"02-02_10-54__L2O__L2O_Update\",\n",
    "        \"ckpt.pth\"\n",
    "        # \"meta_training\",\n",
    "        # \"20.pt\",\n",
    "    ),\n",
    "    map_location=torch.device(DEVICE),\n",
    "    pickle_module=dill,\n",
    ")\n",
    "l2o_dict_2 = ckpt_2[\"l2o_dict\"]\n",
    "l2o_dict_best_2 = ckpt_2[\"l2o_dict_best\"]\n",
    "log_2 = ckpt_2[\"log\"]\n",
    "config_2 = ckpt_2[\"config\"]\n",
    "print(json.dumps(config_2, indent=4, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min_{x} f(x) = \\frac{1}{2} ||W^T x - y||_2^2 \\\\\n",
    "    \\text{where } W \\in \\mathbb{R}^{d \\times m}, y \\in \\mathbb{R}^m\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"time\": datetime.now().strftime(\"%d-%m_%H-%M\"),\n",
    "}\n",
    "\n",
    "### data (task)\n",
    "config[\"data\"] = {\n",
    "    \"d\": 100,\n",
    "    \"m\": 100,\n",
    "    \"data_cls\": CustomTask,\n",
    "}\n",
    "config[\"data\"][\"data_config\"] = {\n",
    "    \"task\": generate_least_squares_task,\n",
    "    \"task_config\": {\n",
    "        \"d\": config[\"data\"][\"d\"],\n",
    "        \"m\": config[\"data\"][\"m\"],\n",
    "        \"verbose\": False,\n",
    "        \"device\": DEVICE,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "### optimizee\n",
    "config[\"optee\"] = {\n",
    "    \"optee_cls\": CustomParams,\n",
    "    \"optee_config\": {\n",
    "        \"dim\": (1, config[\"data\"][\"d\"]),\n",
    "        \"init_params\": \"randn\",\n",
    "    },\n",
    "}\n",
    "\n",
    "### optimizer L2O-CFGD\n",
    "# config[\"opter\"] = {\n",
    "#     \"opter_cls\": L2O,\n",
    "#     \"opter_config\": {\n",
    "#         \"in_dim\": 3, # len(in_features) + 1\n",
    "#         \"out_dim\": 3,\n",
    "#         \"hidden_sz\": 40,\n",
    "#         \"in_features\": (\"grad\", \"iter_num_enc\"),\n",
    "#         \"base_opter_cls\": CFGD_ClosedForm,\n",
    "#         \"base_opter_config\": {\n",
    "#             \"lr\": get_optimal_lr,\n",
    "#             \"gamma\": None,\n",
    "#             \"c\": None,\n",
    "#             \"version\": \"NA\",\n",
    "#             \"init_points\": [torch.randn(1, config[\"data\"][\"d\"], requires_grad=False, device=DEVICE)],\n",
    "#             \"device\": DEVICE,\n",
    "#         },\n",
    "#         \"params_to_optimize\": {\n",
    "#             # \"gamma\": {\n",
    "#             #     \"idx\": 0,\n",
    "#             #     \"act_fns\": (\"identity\", \"diag\"),\n",
    "#             # },\n",
    "#             # \"gamma\": {\n",
    "#             #     \"idx\": 0,\n",
    "#             #     \"act_fns\": (\"alpha_to_gamma\", \"diag\"),\n",
    "#             #     \"beta\": 0.,\n",
    "#             # },\n",
    "#             \"gamma\": {\n",
    "#                 \"idx\": (0, 1),\n",
    "#                 \"act_fns\": (\"alpha_beta_to_gamma\", \"diag\"),\n",
    "#             },\n",
    "#             \"c\": {\n",
    "#                 \"idx\": 2,\n",
    "#                 \"act_fns\": (\"identity\",),\n",
    "#             },\n",
    "#         },\n",
    "#     },\n",
    "# }\n",
    "\n",
    "### optimizer L2O\n",
    "config[\"opter\"] = {\n",
    "    \"opter_cls\": L2O,\n",
    "    \"opter_config\": {\n",
    "        \"in_dim\": 3, # len(in_features) + 1\n",
    "        \"out_dim\": 1,\n",
    "        \"hidden_sz\": 40,\n",
    "        \"in_features\": (\"grad\", \"iter_num_enc\"),\n",
    "        \"base_opter_cls\": L2O_Update,\n",
    "        \"base_opter_config\": {\n",
    "            \"lr\": get_optimal_lr,\n",
    "            \"device\": DEVICE,\n",
    "        },\n",
    "        \"params_to_optimize\": {\n",
    "            \"update\": {\n",
    "                \"idx\": 0,\n",
    "                \"act_fns\": (\"identity\",),\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "### meta-training config\n",
    "config[\"meta_training_config\"] = {\n",
    "    \"meta_opter_cls\": optim.Adam,\n",
    "    \"meta_opter_config\": {\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "    \"n_runs\": 2000,\n",
    "    \"unroll\": 20,\n",
    "    \"loggers\": [\n",
    "        # {\n",
    "        #     \"every_nth_run\": 20,\n",
    "        #     \"logger_fn\": partial(plotter, to_plot=\"gamma\"),\n",
    "        # },\n",
    "        # {\n",
    "        #     \"every_nth_run\": 20,\n",
    "        #     \"logger_fn\": partial(plotter, to_plot=\"c\"),\n",
    "        # }\n",
    "    ],\n",
    "}\n",
    "\n",
    "### other\n",
    "config.update({\n",
    "    \"n_iters\": 800,\n",
    "    \"l2o_dict\": None,\n",
    "    \"additional_metrics\": {\n",
    "        # \"gamma\": lambda opter, **kwargs: \\\n",
    "        #     # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "        #     opter.base_opter.param_groups[0][\"gamma\"].mean().item() \\\n",
    "        #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"gamma\", None),\n",
    "        # \"c\": lambda opter, **kwargs: \\\n",
    "        #     # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "        #     opter.base_opter.param_groups[0][\"c\"].mean().item() \\\n",
    "        #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"c\", None),\n",
    "        # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "        #     torch.norm(task[\"x_tik_solution\"](gamma=1., c=1) - optee.params.detach(), p=2).item(),\n",
    "        # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "        #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "    },\n",
    "    \"ckpt_config\": {\n",
    "        \"ckpt_every_nth_run\": 50,\n",
    "        \"ckpt_dir\": os.path.join(\n",
    "            CKPT_PATH,\n",
    "            \"l2o\",\n",
    "            config[\"time\"] + \"__\"\\\n",
    "                + config[\"opter\"][\"opter_cls\"].__name__ + \"__\"\\\n",
    "                + config[\"opter\"][\"opter_config\"][\"base_opter_cls\"].__name__,\n",
    "        ),\n",
    "    },\n",
    "    \"device\": DEVICE,\n",
    "    \"seed\": 0,\n",
    "})\n",
    "config[\"ckpt_config\"][\"ckpt_dir_meta_training\"] = os.path.join(\n",
    "    config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "    \"meta_training\",\n",
    ")\n",
    "config[\"ckpt_config\"][\"ckpt_dir_meta_testing\"] = os.path.join(\n",
    "    config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "    \"meta_testing\",\n",
    ")\n",
    "\n",
    "### make dirs\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir\"], exist_ok=True)\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir_meta_training\"], exist_ok=True)\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir_meta_testing\"], exist_ok=True)\n",
    "\n",
    "### save config\n",
    "with open(os.path.join(config[\"ckpt_config\"][\"ckpt_dir\"], \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4, default=str)\n",
    "\n",
    "print(f\"Path to checkpoints: {config['ckpt_config']['ckpt_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta train\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "l2o_dict, l2o_dict_best, log = meta_train(\n",
    "    config=config,\n",
    "    ### keep meta-training\n",
    "    # l2o_dict=l2o_dict,\n",
    "    # l2o_dict_best=l2o_dict_best,\n",
    "    # log=log,\n",
    ")\n",
    "\n",
    "### save final checkpoint\n",
    "torch.save({\n",
    "    \"l2o_dict\": l2o_dict,\n",
    "    \"l2o_dict_best\": l2o_dict_best,\n",
    "    \"log\": log,\n",
    "    \"config\": config,\n",
    "}, os.path.join(config[\"ckpt_config\"][\"ckpt_dir\"], \"ckpt.pth\"), pickle_module=dill)\n",
    "\n",
    "plt.plot(log[\"loss_sum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta-testing config\n",
    "test_d, test_m = 100, 100\n",
    "n_test_runs = 15\n",
    "test_run_iters = 500\n",
    "test_runs_seed = 1\n",
    "c_palette = list(plt.cm.tab10.colors)\n",
    "\n",
    "runs = dict()\n",
    "\n",
    "update_config_base = dict()\n",
    "update_config_base[\"n_iters\"] = test_run_iters\n",
    "update_config_base[\"data\"] = {\n",
    "    \"d\": test_d,\n",
    "    \"m\": test_m,\n",
    "    \"data_cls\": CustomTask,\n",
    "    \"data_config\": {\n",
    "        \"task\": generate_least_squares_task,\n",
    "        \"task_config\": {\n",
    "            \"d\": test_d,\n",
    "            \"m\": test_m,\n",
    "            \"verbose\": False,\n",
    "            \"device\": DEVICE,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "update_config_base[\"optee\"] = {\n",
    "    \"optee_cls\": CustomParams,\n",
    "    \"optee_config\": {\n",
    "        \"dim\": (1, test_d),\n",
    "        \"init_params\": \"randn\",\n",
    "    },\n",
    "}\n",
    "update_config_base[\"additional_metrics\"] = {\n",
    "    # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "    #     torch.norm(task[\"x_tik_solution\"](gamma=gamma, c=1) - optee.params.detach(), p=2).item(),\n",
    "    # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "    #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "    # \"x\": lambda y_hat, y, optee: optee.params.detach().cpu().numpy(),\n",
    "    \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "        torch.cosine_similarity(\n",
    "            opter.state[0][\"last_update\"].flatten() if \"last_update\" in opter.state[0] else opter.param_groups[0][\"last_update\"].flatten(),\n",
    "            opter.state[0][\"last_grad\"].flatten() if \"last_grad\" in opter.state[0] else opter.param_groups[0][\"last_grad\"].flatten(),\n",
    "            dim=0\n",
    "        ).item(),\n",
    "    \"last_lr\": lambda opter, **kwargs: \\\n",
    "        opter.state[0][\"last_lr\"] if \"last_lr\" in opter.state[0] else opter.param_groups[0][\"last_lr\"],\n",
    "}\n",
    "\n",
    "\n",
    "runs[\"GD\"] = {\n",
    "    \"update_config\": {\n",
    "        **update_config_base,\n",
    "        \"opter\": {\n",
    "            \"opter_cls\": GD,\n",
    "            \"opter_config\": {\n",
    "                \"lr\": get_optimal_lr,\n",
    "                \"device\": DEVICE,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"plot_config\": {\n",
    "        \"color\": \"black\",\n",
    "        \"linestyle\": \"dashed\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# runs[\"Adam\"] = {\n",
    "#     \"update_config\": {\n",
    "#         **update_config_base,\n",
    "#         \"opter\": {\n",
    "#             \"opter_cls\": Adam,\n",
    "#             \"opter_config\": {\n",
    "#                 \"lr\": get_optimal_lr,\n",
    "#                 \"device\": DEVICE,\n",
    "#             },\n",
    "#         },\n",
    "#     },\n",
    "#     \"plot_config\": {\n",
    "#         \"color\": \"gray\",\n",
    "#         \"linestyle\": \"dashed\",\n",
    "#     },\n",
    "# }\n",
    "\n",
    "for gamma in [0.15]:\n",
    "    runs[r\"NA-CFGD, $\\gamma$=\" + str(gamma)] = {\n",
    "        \"update_config\": {\n",
    "            **update_config_base,\n",
    "            \"opter\": {\n",
    "                \"opter_cls\": CFGD_ClosedForm,\n",
    "                \"opter_config\": {\n",
    "                    \"lr\": get_optimal_lr,\n",
    "                    \"gamma\": gamma,\n",
    "                    \"c\": 1.,\n",
    "                    \"version\": \"NA\",\n",
    "                    \"init_points\": None,\n",
    "                    \"device\": DEVICE,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"plot_config\": {\n",
    "            \"linestyle\": \"dashed\",\n",
    "            \"color\": c_palette[1],\n",
    "        },\n",
    "    }\n",
    "\n",
    "for gamma in [-0.15]:\n",
    "    runs[r\"AT-CFGD, $\\gamma$=\" + str(gamma)] = {\n",
    "        \"update_config\": {\n",
    "            **update_config_base,\n",
    "            \"opter\": {\n",
    "                \"opter_cls\": CFGD_ClosedForm,\n",
    "                \"opter_config\": {\n",
    "                    \"lr\": get_optimal_lr,\n",
    "                    \"gamma\": gamma,\n",
    "                    \"c\": None,\n",
    "                    \"version\": \"AT\",\n",
    "                    \"init_points\": [torch.randn(1, test_d, requires_grad=False, device=DEVICE)],\n",
    "                    \"device\": DEVICE,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"plot_config\": {\n",
    "            \"linestyle\": \"dashed\",\n",
    "            \"color\": c_palette[0],\n",
    "        },\n",
    "    }\n",
    "\n",
    "# runs[\"L2O\"] = {\n",
    "#     \"update_config\": {\n",
    "#         \"n_iters\": update_config_base[\"n_iters\"],\n",
    "#         \"data\": update_config_base[\"data\"],\n",
    "#         \"optee\": update_config_base[\"optee\"],\n",
    "#         \"additional_metrics\": {\n",
    "#             \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "#                 torch.cosine_similarity(\n",
    "#                     opter.base_opter.state[0][\"last_update\"].flatten(),\n",
    "#                     opter.base_opter.state[0][\"last_grad\"].flatten(),\n",
    "#                     dim=0\n",
    "#                 ).item(),\n",
    "#             \"last_lr\": lambda opter, **kwargs: \\\n",
    "#                 opter.base_opter.state[0][\"last_lr\"].item() if type(opter.base_opter.state[0][\"last_lr\"]) == torch.Tensor else opter.base_opter.state[0][\"last_lr\"],\n",
    "#         },\n",
    "#         # \"l2o_dict\": l2o_dict_2,\n",
    "#         \"l2o_dict\": l2o_dict_best_2[\"best_l2o_dict\"],\n",
    "#     },\n",
    "#     \"plot_config\": {\n",
    "#         \"linestyle\": \"dashed\",\n",
    "#         \"color\": c_palette[2],\n",
    "#     },\n",
    "# }\n",
    "\n",
    "runs[\"L2O-CFGD\"] = {\n",
    "    \"update_config\": {\n",
    "        \"n_iters\": test_run_iters,\n",
    "        \"data\": update_config_base[\"data\"],\n",
    "        \"optee\": update_config_base[\"optee\"],\n",
    "        \"additional_metrics\": {\n",
    "            # \"gamma\": lambda opter, **kwargs: \\\n",
    "            #     # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "            #     opter.base_opter.param_groups[0][\"gamma\"].detach().cpu().numpy() \\\n",
    "            #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"gamma\", None),\n",
    "            \"c\": lambda opter, **kwargs: \\\n",
    "                # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "                opter.base_opter.param_groups[0][\"c\"].detach().cpu().numpy() \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"c\", None),\n",
    "            \"alpha\": lambda opter, **kwargs: \\\n",
    "                # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "                opter.base_opter.param_groups[0][\"alpha\"] \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"alpha\", None),\n",
    "            \"beta\": lambda opter, **kwargs: \\\n",
    "                # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "                opter.base_opter.param_groups[0][\"beta\"] \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"beta\", None),\n",
    "            \"grad\": lambda opter, **kwargs: \\\n",
    "                opter.base_opter.state[0][\"last_grad\"].detach().cpu().numpy() \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.state[0][\"last_grad\"].detach().cpu().numpy(),\n",
    "            \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "                torch.cosine_similarity(\n",
    "                    opter.base_opter.state[0][\"last_update\"].flatten(),\n",
    "                    opter.base_opter.state[0][\"last_grad\"].flatten(),\n",
    "                    dim=0\n",
    "                ).item(),\n",
    "            \"last_lr\": lambda opter, **kwargs: \\\n",
    "                opter.base_opter.state[0][\"last_lr\"].item() if type(opter.base_opter.state[0][\"last_lr\"]) == torch.Tensor else opter.base_opter.state[0][\"last_lr\"],\n",
    "        },\n",
    "        # \"l2o_dict\": l2o_dict,\n",
    "        \"l2o_dict\": l2o_dict_best[\"best_l2o_dict\"],\n",
    "    },\n",
    "    \"plot_config\": {\n",
    "        # \"color\": \"orange\",\n",
    "        \"color\": c_palette[3],\n",
    "        # \"linewidth\": 1.5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run all\n",
    "for run_name in runs.keys():\n",
    "    ### remove all non-alphanumeric characters from run_name\n",
    "    run_name_clean = ''.join(e for e in run_name if e.isalnum())\n",
    "    run_nickname = f\"{run_name_clean}__{test_d}d_{test_m}m_{n_test_runs}runs_{test_run_iters}iters_{test_runs_seed}seed\"\n",
    "    \n",
    "    run_config = copy.deepcopy(config)\n",
    "    if \"update_config\" in runs[run_name] and runs[run_name][\"update_config\"] is not None:\n",
    "        run_config.update(runs[run_name][\"update_config\"])\n",
    "    print(f\"{run_name}:\")\n",
    "    \n",
    "    ### check if already run    \n",
    "    # already loaded?\n",
    "    if \"log\" in runs[run_name]:\n",
    "        print(\"  > Already run.\")\n",
    "        continue\n",
    "\n",
    "    # already saved?\n",
    "    if \"l2o\" in run_name.lower():\n",
    "        if CKPT_PATH in run_config[\"ckpt_config\"][\"ckpt_dir\"]:\n",
    "            save_to = os.path.join(run_config[\"ckpt_config\"][\"ckpt_dir_meta_testing\"], run_nickname + \".pt\")\n",
    "        else:\n",
    "            ckpt_dir_local = os.path.basename(run_config[\"ckpt_config\"][\"ckpt_dir\"])\n",
    "            save_to = os.path.join(CKPT_PATH, \"l2o\", ckpt_dir_local, \"meta_testing\", run_nickname + \".pt\")\n",
    "    else: # baseline\n",
    "        save_to = os.path.join(CKPT_PATH, \"baselines\",  run_nickname + \".pt\")\n",
    "    if os.path.exists(save_to):\n",
    "        print(f\"  > Already saved. Only loading...\\n  > {save_to}\")\n",
    "        runs[run_name] = torch.load(save_to, map_location=torch.device(DEVICE), pickle_module=dill)\n",
    "        continue\n",
    "\n",
    "    torch.manual_seed(test_runs_seed)\n",
    "    np.random.seed(test_runs_seed)\n",
    "\n",
    "    if \"lr\" in run_config[\"opter\"][\"opter_config\"] \\\n",
    "        and run_config[\"opter\"][\"opter_config\"][\"lr\"] == find_best_lr:\n",
    "        print(\"  > Finding best lr...\")\n",
    "        run_config[\"opter\"][\"opter_config\"][\"lr\"] = find_best_lr(\n",
    "            opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "            opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "            optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "            optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "            data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "            data_config=run_config[\"data\"][\"data_config\"],\n",
    "            # loss_fn=run_config[\"loss_fn\"],\n",
    "            n_iters=120,\n",
    "            n_tests=1,\n",
    "            consider_metric=\"loss\",\n",
    "            lrs_to_try=[0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0],\n",
    "        )\n",
    "        print(f\"  > Best lr: {run_config['opter']['opter_config']['lr']}\")\n",
    "\n",
    "    print(\"  > Running...\")\n",
    "    runs[run_name][\"log\"] = dict()\n",
    "    for i in range(n_test_runs):\n",
    "        print(f\"    > Run {i+1}/{n_test_runs}...\")\n",
    "\n",
    "        ### check if L2O has been meta-trained\n",
    "        assert not run_config[\"opter\"][\"opter_cls\"] == L2O or run_config[\"l2o_dict\"] is not None\n",
    "\n",
    "        curr_log = do_fit(\n",
    "            opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "            opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "            optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "            optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "            data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "            data_config=run_config[\"data\"][\"data_config\"],\n",
    "            n_iters=run_config[\"n_iters\"],\n",
    "            l2o_dict=run_config[\"l2o_dict\"],\n",
    "            in_meta_training=False,\n",
    "            additional_metrics=run_config[\"additional_metrics\"],\n",
    "        )[0]\n",
    "\n",
    "        for metric_name in curr_log.keys():\n",
    "            if metric_name not in runs[run_name][\"log\"]:\n",
    "                runs[run_name][\"log\"][metric_name] = []\n",
    "            runs[run_name][\"log\"][metric_name].append(curr_log[metric_name])\n",
    "\n",
    "    runs[run_name][\"config\"] = run_config\n",
    "\n",
    "    ### save results\n",
    "    torch.save(runs[run_name], save_to, pickle_module=dill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_publication_plt_settings(font_size=16, dpi=600, figsize=(5, 3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting config\n",
    "metric = \"loss\"\n",
    "show_max_iters = test_run_iters\n",
    "log_metric = True\n",
    "conv_window = 5\n",
    "\n",
    "### where to save the figure\n",
    "fig_dir = \"../results/publication/quadratic/src\"\n",
    "fig_name = f\"{metric}_{test_d}d_{test_m}m_{show_max_iters}iters.pdf\"\n",
    "if log_metric:\n",
    "    fig_name = f\"log_{fig_name}\"\n",
    "save_fig_to_path = os.path.join(fig_dir, fig_name)\n",
    "# save_fig_to_path = None # don't save\n",
    "print(f\"Final destination: {save_fig_to_path if save_fig_to_path is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(\n",
    "    baselines={\n",
    "        k: r for k, r in runs.items() if \"L2O-CFGD\" not in k\n",
    "    },\n",
    "    l2os={\n",
    "        k: r for k, r in runs.items() if \"L2O-CFGD\" in k\n",
    "    },\n",
    "    metric=metric,\n",
    "    show_max_iters=show_max_iters,\n",
    "    log_metric=log_metric,\n",
    "    with_err_bars=True,\n",
    "    conv_window=conv_window,\n",
    "    save_fig_to_path=save_fig_to_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log(\n",
    "    runs,\n",
    "    only_metrics=[\"loss\", \"cos_sim(d, x.grad)\", \"last_lr\"],\n",
    "    log_metrics=[\"loss\", \"l2_dist(x_tik*, x)\", \"l2_dist(x*, x)\"],\n",
    "    conv_win=1,\n",
    "    min_max_y_config={\n",
    "        \"last_lr\": (0, 100),\n",
    "    },\n",
    "    # save_to=os.path.join(\n",
    "    #     config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "    #     f\"loss_cos_sim_l2o_best_dict_{test_d}d_{test_m}m_{n_test_runs}runs_{test_run_iters}iters.png\"\n",
    "    # ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting config\n",
    "to_plot_label = r\"$c$\"\n",
    "to_plot_label_cleaned = ''.join(e for e in to_plot_label if e.isalnum())\n",
    "to_plot = np.stack(runs[\"L2O-CFGD\"][\"log\"][to_plot_label_cleaned])[0].squeeze()  # (n_test_runs, n_iters, D)\n",
    "\n",
    "### where to save the figure\n",
    "fig_dir = \"../results/publication/quadratic/src\"\n",
    "fig_name = f\"{to_plot_label_cleaned}_{test_d}d_{test_m}m_{test_run_iters}iters.pdf\"\n",
    "save_fig_to_path = os.path.join(fig_dir, fig_name)\n",
    "# save_fig_to_path = None # don't save\n",
    "print(f\"Final destination: {save_fig_to_path if save_fig_to_path is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_strategy(\n",
    "    to_plot=to_plot,\n",
    "    y_label=to_plot_label,\n",
    "    save_fig_to_path=save_fig_to_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple parameters\n",
    "- How do alphas, betas, cs and grads correlate with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.stack(runs[\"L2O-CFGD\"][\"log\"][\"alpha\"]).reshape(n_test_runs, test_run_iters, -1)\n",
    "betas = np.stack(runs[\"L2O-CFGD\"][\"log\"][\"beta\"]).reshape(n_test_runs, test_run_iters, -1)\n",
    "cs = np.stack(runs[\"L2O-CFGD\"][\"log\"][\"c\"]).reshape(n_test_runs, test_run_iters, -1)  # (n_test_runs, n_iters, D)\n",
    "grads = np.stack(runs[\"L2O-CFGD\"][\"log\"][\"grad\"]).reshape(n_test_runs, test_run_iters, -1)  # (n_test_runs, n_iters, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_components = 200\n",
    "component_idxs = np.random.choice(alphas.shape[-1], size=max_components, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run_idx = 0\n",
    "iters_to_show = [0, 1, 2, 5, 20]\n",
    "\n",
    "# fig = plt.figure(figsize=(14, 18), facecolor=\"white\")\n",
    "fig = plt.figure()\n",
    "# fig.suptitle(\"L2O + CFGD_ClosedForm\")\n",
    "ax_idx = 1\n",
    "\n",
    "for i, iter_idx in enumerate(iters_to_show):\n",
    "    ax = fig.add_subplot(len(iters_to_show), 3, ax_idx)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    \n",
    "    ### alphas\n",
    "    sns.scatterplot(\n",
    "        x=grads[test_run_idx, iter_idx, component_idxs],\n",
    "        y=alphas[test_run_idx, iter_idx, component_idxs],\n",
    "        ax=ax,\n",
    "    )\n",
    "    if i == len(iters_to_show) - 1:\n",
    "        ax.set_xlabel(r\"$\\partial_{x_j} f(x_j)$\")\n",
    "    ax.set_ylabel(r\"$\\alpha$\")\n",
    "    # ax.set_title(fr\"Iteration {iter_idx}\")\n",
    "\n",
    "    ### betas\n",
    "    ax = fig.add_subplot(len(iters_to_show), 3, ax_idx + 1)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    sns.scatterplot(\n",
    "        x=grads[test_run_idx, iter_idx, component_idxs],\n",
    "        y=betas[test_run_idx, iter_idx, component_idxs],\n",
    "        ax=ax,\n",
    "    )\n",
    "    if i == len(iters_to_show) - 1:\n",
    "        ax.set_xlabel(r\"$\\partial_{x_j} f(x_j)$\")\n",
    "    ax.set_ylabel(r\"$\\beta$\")\n",
    "    ax.set_title(fr\"Iteration {iter_idx}\", pad=10)\n",
    "    # ax.set_title(fr\"$\\beta$ (iter {iter_idx})\")\n",
    "\n",
    "    ### cs\n",
    "    ax = fig.add_subplot(len(iters_to_show), 3, ax_idx + 2)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    sns.scatterplot(\n",
    "        x=grads[test_run_idx, iter_idx, component_idxs],\n",
    "        y=cs[test_run_idx, iter_idx, component_idxs],\n",
    "        ax=ax,\n",
    "    )\n",
    "    if i == len(iters_to_show) - 1:\n",
    "        ax.set_xlabel(r\"$\\partial_{x_j} f(x_j)$\")\n",
    "    ax.set_ylabel(r\"$c$\")\n",
    "    # ax.set_title(fr\"$c$ (iter {iter_idx})\")\n",
    "\n",
    "    ax_idx += 3\n",
    "\n",
    "# fig.tight_layout(h_pad=1.5)\n",
    "# save_to = os.path.join(\n",
    "#     config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "#     f\"strategy_grad_alpha_beta_c_{test_d}d_{test_m}m_{n_test_runs}runs_{test_run_iters}iters.png\"\n",
    "# )\n",
    "# fig.savefig(save_to)\n",
    "plt.tight_layout(h_pad=2.2)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"../results/strategy.eps\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Objective Function (Figure 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quadratic objective function\n",
    "    min_x f(x, y) = 10 * x^2 + y^2\n",
    "\"\"\"\n",
    "def task_gen(device=\"cpu\"):\n",
    "    ### Least squares problem 1/2 x^T A x + b^T x\n",
    "    A = torch.tensor([[10., 0.], [0., 1.]], device=device)\n",
    "    b = torch.tensor([[0., 0.]], device=device).T\n",
    "    x_solution = torch.tensor([0., 0.], device=device)\n",
    "\n",
    "    loss_fn = lambda y_hat: 0.5 * y_hat @ A @ y_hat.T + b.T @ y_hat.T\n",
    "\n",
    "    return {\n",
    "        \"A\": A,\n",
    "        \"b\": b,\n",
    "        \"loss_fn\": loss_fn,\n",
    "        \"x_solution\": x_solution,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"optee\": {\n",
    "        \"optee_cls\": CustomParams,\n",
    "        \"optee_config\": {\n",
    "            \"dim\": (1, 2),\n",
    "            \"init_params\": torch.tensor([[1., -10.]], device=DEVICE),\n",
    "            \"param_func\": None,\n",
    "        },\n",
    "    },\n",
    "    \"opter\": {\n",
    "        \"opter_cls\": CFGD_ClosedForm,\n",
    "        \"opter_config\": {\n",
    "            \"lr\": get_optimal_lr,\n",
    "            \"gamma\": -1.,\n",
    "            \"c\": None,\n",
    "            \"version\": \"AT\",\n",
    "            \"init_points\": [torch.tensor([-1., -1.], device=DEVICE)],\n",
    "            \"device\": DEVICE,\n",
    "        },\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"data_cls\": CustomTask,\n",
    "        \"data_config\": {\n",
    "            \"task\": task_gen,\n",
    "            \"task_config\": {\n",
    "                # \"verbose\": False,\n",
    "                \"device\": DEVICE,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"n_iters\": 50,\n",
    "    \"l2o_dict\": None,\n",
    "    \"additional_metrics\": {\n",
    "        \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "            torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "    },\n",
    "    \"device\": DEVICE,\n",
    "    \"seed\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### runs config\n",
    "runs = dict()\n",
    "\n",
    "runs[\"GD\"] = {\n",
    "    \"update_config\": {\n",
    "        \"opter\": {\n",
    "            \"opter_cls\": GD,\n",
    "            \"opter_config\": {\n",
    "                \"lr\": get_optimal_lr,\n",
    "                \"device\": DEVICE,\n",
    "            },\n",
    "        },\n",
    "        \"additional_metrics\": {\n",
    "            \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "                torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "            \"x\": lambda optee, **kwargs: optee.params.detach().cpu().numpy(),\n",
    "        },\n",
    "    },\n",
    "    \"plot_config\": {\n",
    "        \"color\": \"black\",\n",
    "        \"linestyle\": \"dashed\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# for gamma in [-1., 0.01, 0.05, 0.25, 0.5, 1., 10.]:\n",
    "for gamma in [0.05]:\n",
    "    runs[r\"CFGD_ClosedForm, $\\gamma$=\" + str(gamma)] = {\n",
    "        \"update_config\": {\n",
    "            \"opter\": {\n",
    "                \"opter_cls\": CFGD_ClosedForm,\n",
    "                \"opter_config\": {\n",
    "                    \"lr\": get_optimal_lr,\n",
    "                    \"gamma\": gamma,\n",
    "                    \"c\": None,\n",
    "                    \"version\": \"AT\",\n",
    "                    \"init_points\": [torch.tensor([-1., -1.], device=DEVICE)],\n",
    "                    \"device\": DEVICE,\n",
    "                },\n",
    "            },\n",
    "            \"additional_metrics\": {\n",
    "                \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "                    torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "                \"x\": lambda optee, **kwargs: optee.params.detach().cpu().numpy(),\n",
    "            },\n",
    "        },\n",
    "        # \"plot_config\": {\n",
    "        #     \"linestyle\": \"dashed\",\n",
    "        # },\n",
    "    }\n",
    "\n",
    "runs[\"L2O-CFGD\"] = {\n",
    "    \"update_config\": {\n",
    "        \"opter\": {\n",
    "            \"opter_cls\": L2O,\n",
    "            \"opter_config\": {\n",
    "                \"in_dim\": 3, # len(in_features) + 1\n",
    "                \"out_dim\": 3,\n",
    "                \"hidden_sz\": 40,\n",
    "                \"in_features\": (\"grad\", \"iter_num_enc\"),\n",
    "                \"base_opter_cls\": CFGD_ClosedForm,\n",
    "                \"base_opter_config\": {\n",
    "                    \"lr\": get_optimal_lr,\n",
    "                    \"gamma\": None,\n",
    "                    \"c\": None,\n",
    "                    \"version\": \"NA\",\n",
    "                    # \"init_points\": [torch.randn(1, config[\"data\"][\"d\"], requires_grad=False, device=DEVICE)],\n",
    "                    \"device\": DEVICE,\n",
    "                },\n",
    "                \"params_to_optimize\": {\n",
    "                    # \"gamma\": {\n",
    "                    #     \"idx\": 0,\n",
    "                    #     \"act_fns\": (\"identity\", \"diag\"),\n",
    "                    # },\n",
    "                    # \"gamma\": {\n",
    "                    #     \"idx\": 0,\n",
    "                    #     \"act_fns\": (\"alpha_to_gamma\", \"diag\"),\n",
    "                    #     \"beta\": 0.,\n",
    "                    # },\n",
    "                    \"gamma\": {\n",
    "                        \"idx\": (0, 1),\n",
    "                        \"act_fns\": (\"alpha_beta_to_gamma\", \"diag\"),\n",
    "                    },\n",
    "                    \"c\": {\n",
    "                        \"idx\": 2,\n",
    "                        \"act_fns\": (\"identity\",),\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"additional_metrics\": {\n",
    "            # \"gamma\": lambda opter, **kwargs: \\\n",
    "            #     # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "            #     opter.base_opter.param_groups[0][\"gamma\"].detach().cpu().numpy() \\\n",
    "            #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"gamma\", None),\n",
    "            # \"c\": lambda opter, **kwargs: \\\n",
    "            #     # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "            #     opter.base_opter.param_groups[0][\"c\"].detach().cpu().numpy() \\\n",
    "            #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"c\", None),\n",
    "            # \"alpha\": lambda opter, **kwargs: \\\n",
    "            #     # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "            #     opter.base_opter.param_groups[0][\"alpha\"] \\\n",
    "            #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"alpha\", None),\n",
    "            # \"beta\": lambda opter, **kwargs: \\\n",
    "            #     # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "            #     opter.base_opter.param_groups[0][\"beta\"] \\\n",
    "            #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"beta\", None),\n",
    "            # \"grad\": lambda opter, **kwargs: \\\n",
    "            #     opter.base_opter.state[0][\"last_grad\"].detach().cpu().numpy() \\\n",
    "            #     if hasattr(opter, \"base_opter\") else opter.state[0][\"last_grad\"].detach().cpu().numpy(),\n",
    "            \"x\": lambda optee, **kwargs: optee.params.detach().cpu().numpy(),\n",
    "            \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "                torch.cosine_similarity(\n",
    "                    opter.base_opter.state[0][\"last_update\"].flatten(),\n",
    "                    opter.base_opter.state[0][\"last_grad\"].flatten(),\n",
    "                    dim=0\n",
    "                ).item(),\n",
    "            \"last_lr\": lambda opter, **kwargs: \\\n",
    "                opter.base_opter.state[0][\"last_lr\"].item() if type(opter.base_opter.state[0][\"last_lr\"]) == torch.Tensor else opter.base_opter.state[0][\"last_lr\"],\n",
    "        },\n",
    "        # \"l2o_dict\": l2o_dict,\n",
    "        \"l2o_dict\": l2o_dict_best[\"best_l2o_dict\"],\n",
    "    },\n",
    "    \"plot_config\": {\n",
    "        # \"color\": \"orange\",\n",
    "        # \"color\": c_palette[3],\n",
    "        # \"linewidth\": 1.5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run all\n",
    "for run_name in runs.keys():\n",
    "    if \"log\" in runs[run_name]:\n",
    "        continue # already run\n",
    "    run_config = copy.deepcopy(config)\n",
    "    if \"update_config\" in runs[run_name] and runs[run_name][\"update_config\"] is not None:\n",
    "        run_config.update(runs[run_name][\"update_config\"])\n",
    "    print(f\"{run_name}:\")\n",
    "    \n",
    "    torch.manual_seed(config[\"seed\"])\n",
    "    np.random.seed(config[\"seed\"])\n",
    "\n",
    "    if \"lr\" in run_config[\"opter\"][\"opter_config\"] \\\n",
    "        and run_config[\"opter\"][\"opter_config\"][\"lr\"] == find_best_lr:\n",
    "        print(\"  > Finding best lr...\")\n",
    "        run_config[\"opter\"][\"opter_config\"][\"lr\"] = find_best_lr(\n",
    "            opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "            opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "            optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "            optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "            data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "            data_config=run_config[\"data\"][\"data_config\"],\n",
    "            # loss_fn=run_config[\"loss_fn\"],\n",
    "            n_iters=120,\n",
    "            n_tests=1,\n",
    "            consider_metric=\"loss\",\n",
    "            lrs_to_try=[0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0],\n",
    "        )\n",
    "        print(f\"  > Best lr: {run_config['opter']['opter_config']['lr']}\")\n",
    "\n",
    "    print(\"  > Running...\")\n",
    "    runs[run_name][\"log\"], _, _ = do_fit(\n",
    "        opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "        opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "        optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "        optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "        data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "        data_config=run_config[\"data\"][\"data_config\"],\n",
    "        n_iters=run_config[\"n_iters\"],\n",
    "        l2o_dict=run_config[\"l2o_dict\"],\n",
    "        in_meta_training=False,\n",
    "        additional_metrics=run_config[\"additional_metrics\"],\n",
    "    )\n",
    "    runs[run_name][\"config\"] = run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log(\n",
    "    runs,\n",
    "    only_metrics=[\"loss\", \"l2_dist(x*, x)\", \"time\"],\n",
    "    log_metrics=[\"loss\", \"l2_dist(x_tik*, x)\", \"l2_dist(x*, x)\"],\n",
    "    conv_win=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x, y):\n",
    "    return 10 * x**2 + y**2\n",
    "\n",
    "def plot_optimizer_steps(runs, num_steps=10, only_opters=None, starting_points=None):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Plot the contour of the objective function\n",
    "    x_range = np.linspace(-1.5, 1.5, 100)\n",
    "    y_range = np.linspace(-10.5, 2, 100)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = objective_function(X, Y)\n",
    "    contour = plt.contour(Y, X, Z, levels=20, cmap='viridis')\n",
    "    plt.colorbar(contour, label='Objective Function Value')\n",
    "\n",
    "    for optimizer, data in runs.items():\n",
    "        if only_opters is not None and optimizer not in only_opters:\n",
    "            continue\n",
    "\n",
    "\n",
    "        x_values = [x.flatten() for x in data[\"log\"][\"x\"][:num_steps]]\n",
    "        if starting_points is not None:\n",
    "            x_values = starting_points + x_values\n",
    "        x_values = np.stack(x_values)\n",
    "\n",
    "        plt.plot(x_values[:, 1], x_values[:, 0], label=optimizer, marker='o', markersize=8, linewidth=3)\n",
    "\n",
    "    plt.title('Optimizer Steps')\n",
    "    plt.xlabel('y')\n",
    "    plt.ylabel('x')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'runs' with your actual dictionary\n",
    "# For demonstration, I'm using a simplified structure\n",
    "# runs = {\n",
    "#     \"GD\": {\"log\": {\"x\": [torch.randn(1, 2) for _ in range(20)]}},\n",
    "#     \"Adam\": {\"log\": {\"x\": [torch.randn(1, 2) for _ in range(20)]}},\n",
    "#     # Add more optimizers as needed\n",
    "# }\n",
    "\n",
    "plot_optimizer_steps(\n",
    "    runs,\n",
    "    only_opters=[\n",
    "        \"GD\",\n",
    "        \"L2O-CFGD\",\n",
    "        \"CFGD_ClosedForm, $\\\\gamma$=0.05\",\n",
    "        # \"CFGD_ClosedForm, $\\\\gamma$=-1.0\",\n",
    "        # \"CFGD_ClosedForm, $\\\\gamma$=0.5\",\n",
    "        # \"CFGD_ClosedForm, $\\\\gamma$=10.0\"\n",
    "    ],\n",
    "    num_steps=12,\n",
    "    starting_points=[[1., -10.]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"time\": datetime.now().strftime(\"%d-%m_%H-%M\"),\n",
    "}\n",
    "\n",
    "### data (task)\n",
    "config[\"data\"] = {\n",
    "    \"data_cls\": MNIST,\n",
    "}\n",
    "config[\"data\"][\"data_config\"] = {\n",
    "    \"device\": DEVICE,\n",
    "    \"preload\": True,\n",
    "}\n",
    "\n",
    "### optimizee\n",
    "config[\"optee\"] = {\n",
    "    \"optee_cls\": MLPOptee,\n",
    "    \"optee_config\": {\n",
    "        \"layer_sizes\": [20],\n",
    "        \"act_fn\": nn.ReLU(),\n",
    "    },\n",
    "}\n",
    "\n",
    "### optimizer L2O-CFGD\n",
    "config[\"opter\"] = {\n",
    "    \"opter_cls\": L2O,\n",
    "    \"opter_config\": {\n",
    "        \"in_dim\": 3, # len(in_features) + 1\n",
    "        \"out_dim\": 3,\n",
    "        \"hidden_sz\": 40,\n",
    "        \"in_features\": (\"grad\", \"iter_num_enc\"),\n",
    "        \"base_opter_cls\": CFGD,\n",
    "        \"base_opter_config\": {\n",
    "            \"lr\": 0.05,\n",
    "            \"alpha\": None,\n",
    "            \"beta\": None,\n",
    "            \"c\": None,\n",
    "            \"s\": 1,\n",
    "            \"version\": \"NA\",\n",
    "            \"init_points\": None,\n",
    "            \"device\": DEVICE,\n",
    "        },\n",
    "        \"params_to_optimize\": {\n",
    "            \"alpha\": {\n",
    "                \"idx\": 0,\n",
    "                \"act_fns\": (\"sigmoid\",),\n",
    "            },\n",
    "            \"beta\": {\n",
    "                \"idx\": 1,\n",
    "                \"act_fns\": (\"identity\",),\n",
    "            },\n",
    "            \"c\": {\n",
    "                \"idx\": 2,\n",
    "                \"act_fns\": (\"identity\",),\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "### optimizer L2O\n",
    "# config[\"opter\"] = {\n",
    "#     \"opter_cls\": L2O,\n",
    "#     \"opter_config\": {\n",
    "#         \"in_dim\": 3, # len(in_features) + 1\n",
    "#         \"out_dim\": 1,\n",
    "#         \"hidden_sz\": 40,\n",
    "#         \"in_features\": (\"grad\", \"iter_num_enc\"),\n",
    "#         \"base_opter_cls\": L2O_Update,\n",
    "#         \"base_opter_config\": {\n",
    "#             \"lr\": 0.1,\n",
    "#             \"device\": DEVICE,\n",
    "#         },\n",
    "#         \"params_to_optimize\": {\n",
    "#             \"update\": {\n",
    "#                 \"idx\": 0,\n",
    "#                 \"act_fns\": (\"identity\",),\n",
    "#             },\n",
    "#         },\n",
    "#     },\n",
    "# }\n",
    "\n",
    "### meta-training config\n",
    "config[\"meta_training_config\"] = {\n",
    "    \"meta_opter_cls\": optim.Adam,\n",
    "    \"meta_opter_config\": {\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"n_runs\": 500,\n",
    "    \"unroll\": 30,\n",
    "    \"loggers\": [\n",
    "        # {\n",
    "        #     \"every_nth_run\": 20,\n",
    "        #     \"logger_fn\": partial(plotter, to_plot=\"gamma\"),\n",
    "        # },\n",
    "        # {\n",
    "        #     \"every_nth_run\": 20,\n",
    "        #     \"logger_fn\": partial(plotter, to_plot=\"c\"),\n",
    "        # }\n",
    "    ],\n",
    "}\n",
    "\n",
    "### other\n",
    "config.update({\n",
    "    \"n_iters\": 200,\n",
    "    \"l2o_dict\": None,\n",
    "    \"additional_metrics\": {\n",
    "        # \"gamma\": lambda opter, **kwargs: \\\n",
    "        #     # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "        #     opter.base_opter.param_groups[0][\"gamma\"].mean().item() \\\n",
    "        #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"gamma\", None),\n",
    "        # \"c\": lambda opter, **kwargs: \\\n",
    "        #     # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "        #     opter.base_opter.param_groups[0][\"c\"].mean().item() \\\n",
    "        #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"c\", None),\n",
    "        # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "        #     torch.norm(task[\"x_tik_solution\"](gamma=1., c=1) - optee.params.detach(), p=2).item(),\n",
    "        # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "        #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "    },\n",
    "    \"ckpt_config\": {\n",
    "        \"ckpt_every_nth_run\": 20,\n",
    "        \"ckpt_dir\": os.path.join(\n",
    "            CKPT_PATH,\n",
    "            \"l2o\",\n",
    "            config[\"time\"] + \"__\"\\\n",
    "                + config[\"opter\"][\"opter_cls\"].__name__ + \"__\"\\\n",
    "                + config[\"opter\"][\"opter_config\"][\"base_opter_cls\"].__name__,\n",
    "        ),\n",
    "    },\n",
    "    \"device\": DEVICE,\n",
    "    \"seed\": 0,\n",
    "})\n",
    "config[\"ckpt_config\"][\"ckpt_dir_meta_training\"] = os.path.join(\n",
    "    config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "    \"meta_training\",\n",
    ")\n",
    "config[\"ckpt_config\"][\"ckpt_dir_meta_testing\"] = os.path.join(\n",
    "    config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "    \"meta_testing\",\n",
    ")\n",
    "\n",
    "### make dirs\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir\"], exist_ok=True)\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir_meta_training\"], exist_ok=True)\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir_meta_testing\"], exist_ok=True)\n",
    "\n",
    "### save config\n",
    "with open(os.path.join(config[\"ckpt_config\"][\"ckpt_dir\"], \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4, default=str)\n",
    "\n",
    "print(f\"Path to checkpoints: {config['ckpt_config']['ckpt_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta train\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "l2o_dict, l2o_dict_best, log = meta_train(\n",
    "    config=config,    \n",
    "    ### keep meta-training\n",
    "    # l2o_dict=l2o_dict,\n",
    "    # l2o_dict_best=l2o_dict_best,\n",
    "    # log=log,\n",
    ")\n",
    "\n",
    "### save checkpoint\n",
    "torch.save({\n",
    "    \"l2o_dict\": l2o_dict,\n",
    "    \"l2o_dict_best\": l2o_dict_best,\n",
    "    \"log\": log,\n",
    "    \"config\": config,\n",
    "}, os.path.join(config[\"ckpt_config\"][\"ckpt_dir\"], \"ckpt.pt\"), pickle_module=dill)\n",
    "\n",
    "plt.plot(log[\"loss_sum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta-testing config\n",
    "n_test_runs = 5\n",
    "test_run_iters = 1000\n",
    "test_runs_seed = 1\n",
    "c_palette = list(plt.cm.tab10.colors)\n",
    "\n",
    "runs = dict()\n",
    "\n",
    "update_config_base = dict()\n",
    "update_config_base[\"n_iters\"] = test_run_iters\n",
    "update_config_base[\"optee\"] = {\n",
    "    \"optee_cls\": MLPOptee,\n",
    "    \"optee_config\": {\n",
    "        \"layer_sizes\": [20],\n",
    "        \"act_fn\": nn.ReLU(),\n",
    "    },\n",
    "}\n",
    "update_config_base[\"additional_metrics\"] = {\n",
    "    # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "    #     torch.norm(task[\"x_tik_solution\"](gamma=gamma, c=1) - optee.params.detach(), p=2).item(),\n",
    "    # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "    #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "    # \"x\": lambda y_hat, y, optee: optee.params.detach().cpu().numpy(),\n",
    "    \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "        torch.cosine_similarity(\n",
    "            opter.state[0][\"last_update\"].flatten() if \"last_update\" in opter.state[0] else opter.param_groups[0][\"last_update\"].flatten(),\n",
    "            opter.state[0][\"last_grad\"].flatten() if \"last_grad\" in opter.state[0] else opter.param_groups[0][\"last_grad\"].flatten(),\n",
    "            dim=0\n",
    "        ).item(),\n",
    "    # \"last_lr\": lambda opter, **kwargs: \\\n",
    "    #     opter.state[0][\"last_lr\"] if \"last_lr\" in opter.state[0] else opter.param_groups[0][\"last_lr\"],\n",
    "}\n",
    "\n",
    "_tmp_optee = update_config_base[\"optee\"][\"optee_cls\"](**update_config_base[\"optee\"][\"optee_config\"])\n",
    "\n",
    "# runs[\"GD\"] = {\n",
    "#     \"update_config\": {\n",
    "#         **update_config_base,\n",
    "#         \"opter\": {\n",
    "#             \"opter_cls\": GD,\n",
    "#             \"opter_config\": {\n",
    "#                 \"lr\": 0.3,\n",
    "#                 \"device\": DEVICE,\n",
    "#             },\n",
    "#         },\n",
    "#     },\n",
    "#     \"plot_config\": {\n",
    "#         \"color\": \"black\",\n",
    "#         \"linestyle\": \"dashed\",\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# for alpha in [0.95]:\n",
    "# # for alpha in [0.4, 0.7, 0.95]:\n",
    "#     for beta in [0.]:\n",
    "#     # for beta in [0., 0.15, 0.6]:\n",
    "#         runs[r\"NA-CFGD, $\\alpha$=\" + str(alpha) + r\", $\\beta$=\" + str(beta)] = {\n",
    "#             \"update_config\": {\n",
    "#                 **update_config_base,\n",
    "#                 \"opter\": {\n",
    "#                     \"opter_cls\": CFGD,\n",
    "#                     \"opter_config\": {\n",
    "#                         \"lr\": 0.05,\n",
    "#                         \"alpha\": alpha,\n",
    "#                         \"beta\": beta,\n",
    "#                         \"c\": 1.,\n",
    "#                         \"s\": 1,\n",
    "#                         \"version\": \"NA\",\n",
    "#                         \"init_points\": None,\n",
    "#                         \"device\": DEVICE,\n",
    "#                     },\n",
    "#                 },\n",
    "#             },\n",
    "#             \"plot_config\": {\n",
    "#                 \"linestyle\": \"dashed\",\n",
    "#                 \"color\": c_palette[1],\n",
    "#             },\n",
    "#         }\n",
    "\n",
    "# for alpha in [0.9]:\n",
    "# # for alpha in [0.01, 0.4, 0.95]:\n",
    "#     for beta in [-0.2]:\n",
    "#     # for beta in [0, 0.3, 0.95]:\n",
    "#         # for lr in [0.01, 0.1, 0.5, 1.0]:\n",
    "#         runs[r\"AT-CFGD, $\\alpha$=\" + str(alpha) + r\", $\\beta$=\" + str(beta)] = {\n",
    "#             \"update_config\": {\n",
    "#                 **update_config_base,\n",
    "#                 \"opter\": {\n",
    "#                     \"opter_cls\": CFGD,\n",
    "#                     \"opter_config\": {\n",
    "#                         \"lr\": 0.1,\n",
    "#                         \"alpha\": alpha,\n",
    "#                         \"beta\": beta,\n",
    "#                         \"c\": 0,\n",
    "#                         \"s\": 1,\n",
    "#                         \"version\": \"AT\",\n",
    "#                         \"init_points\": [\n",
    "#                             [torch.randn_like(p, requires_grad=False, device=DEVICE)] for _, p in _tmp_optee.all_named_parameters()\n",
    "#                         ],\n",
    "#                         \"device\": DEVICE,\n",
    "#                     },\n",
    "#                 },\n",
    "#             },\n",
    "#             \"plot_config\": {\n",
    "#                 \"linestyle\": \"dashed\",\n",
    "#                 \"color\": c_palette[0],\n",
    "#             },\n",
    "#         }\n",
    "\n",
    "# runs[\"L2O\"] = {\n",
    "#     \"update_config\": {\n",
    "#         \"n_iters\": update_config_base[\"n_iters\"],\n",
    "#         \"optee\": update_config_base[\"optee\"],\n",
    "#         \"additional_metrics\": {\n",
    "#             \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "#                 torch.cosine_similarity(\n",
    "#                     opter.base_opter.state[0][\"last_update\"].flatten(),\n",
    "#                     opter.base_opter.state[0][\"last_grad\"].flatten(),\n",
    "#                     dim=0\n",
    "#                 ).item(),\n",
    "#             \"last_lr\": lambda opter, **kwargs: \\\n",
    "#                 opter.base_opter.state[0][\"last_lr\"].item() if type(opter.base_opter.state[0][\"last_lr\"]) == torch.Tensor else opter.base_opter.state[0][\"last_lr\"],\n",
    "#         },\n",
    "#         \"l2o_dict\": l2o_dict_2,\n",
    "#         # \"l2o_dict\": l2o_dict_best_2[\"best_l2o_dict\"],\n",
    "#     },\n",
    "#     \"plot_config\": {\n",
    "#         \"linestyle\": \"dashed\",\n",
    "#         \"color\": c_palette[2],\n",
    "#     },\n",
    "# }\n",
    "\n",
    "runs[\"L2O-CFGD\"] = {\n",
    "    \"update_config\": {\n",
    "        \"n_iters\": update_config_base[\"n_iters\"],\n",
    "        \"optee\": update_config_base[\"optee\"],\n",
    "        \"additional_metrics\": {\n",
    "            # \"gamma\": lambda opter, **kwargs: \\\n",
    "            #     # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "            #     opter.base_opter.param_groups[0][\"gamma\"].detach().cpu().numpy() \\\n",
    "            #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"gamma\", None),\n",
    "            \"c\": lambda opter, **kwargs: \\\n",
    "                # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "                opter.base_opter.param_groups[0][\"c\"].detach().cpu().numpy() \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"c\", None),\n",
    "            \"alpha\": lambda opter, **kwargs: \\\n",
    "                # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "                opter.base_opter.param_groups[0][\"alpha\"].detach().cpu().numpy() \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"alpha\", None),\n",
    "            \"beta\": lambda opter, **kwargs: \\\n",
    "                # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "                opter.base_opter.param_groups[0][\"beta\"].detach().cpu().numpy() \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"beta\", None),\n",
    "            \"grad\": lambda opter, **kwargs: \\\n",
    "                opter.base_opter.state[0][\"last_grad\"].detach().cpu().numpy() \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.state[0][\"last_grad\"].detach().cpu().numpy(),\n",
    "            \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "                torch.cosine_similarity(\n",
    "                    opter.base_opter.state[0][\"last_update\"].flatten(),\n",
    "                    opter.base_opter.state[0][\"last_grad\"].flatten(),\n",
    "                    dim=0\n",
    "                ).item(),\n",
    "            \"last_lr\": lambda opter, **kwargs: \\\n",
    "                opter.base_opter.state[0][\"last_lr\"].item() if type(opter.base_opter.state[0][\"last_lr\"]) == torch.Tensor else opter.base_opter.state[0][\"last_lr\"],\n",
    "        },\n",
    "        \"l2o_dict\": l2o_dict,\n",
    "        # \"l2o_dict\": l2o_dict_best[\"best_l2o_dict\"],\n",
    "    },\n",
    "    \"plot_config\": {\n",
    "        # \"color\": \"orange\",\n",
    "        \"color\": c_palette[3],\n",
    "        # \"linewidth\": 1.5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run all\n",
    "for run_name in runs.keys():\n",
    "    ### prepare run config\n",
    "    run_config = copy.deepcopy(config)\n",
    "    if \"update_config\" in runs[run_name] and runs[run_name][\"update_config\"] is not None:\n",
    "        run_config.update(runs[run_name][\"update_config\"])\n",
    "    print(f\"{run_name}:\")\n",
    "\n",
    "    ### remove all non-alphanumeric characters from run_name\n",
    "    run_name_clean = ''.join(e for e in run_name if e.isalnum())\n",
    "    run_nickname = f\"{run_name_clean}__{dict_to_str(run_config['optee']['optee_config'])}__{n_test_runs}runs_{test_run_iters}iters_{test_runs_seed}seed\"\n",
    "    \n",
    "    ### check if already run    \n",
    "    # already loaded?\n",
    "    if \"log\" in runs[run_name]:\n",
    "        print(\"  > Already run.\")\n",
    "        continue\n",
    "\n",
    "    # already saved?\n",
    "    if \"l2o\" in run_name.lower():\n",
    "        if CKPT_PATH in run_config[\"ckpt_config\"][\"ckpt_dir\"]:\n",
    "            save_to = os.path.join(run_config[\"ckpt_config\"][\"ckpt_dir_meta_testing\"], run_nickname + \".pt\")\n",
    "        else:\n",
    "            ckpt_dir_local = os.path.basename(run_config[\"ckpt_config\"][\"ckpt_dir\"])\n",
    "            save_to = os.path.join(CKPT_PATH, \"l2o\", ckpt_dir_local, \"meta_testing\", run_nickname + \".pt\")\n",
    "    else: # baseline\n",
    "        save_to = os.path.join(CKPT_PATH, \"baselines\",  run_nickname + \".pt\")\n",
    "    if os.path.exists(save_to):\n",
    "        print(f\"  > Already saved. Only loading...\\n  > {save_to}\")\n",
    "        plot_conf = copy.deepcopy(runs[run_name][\"plot_config\"])\n",
    "        runs[run_name] = torch.load(save_to, map_location=torch.device(DEVICE), pickle_module=dill)\n",
    "        runs[run_name][\"plot_config\"] = plot_conf\n",
    "        continue\n",
    "\n",
    "    torch.manual_seed(test_runs_seed)\n",
    "    np.random.seed(test_runs_seed)\n",
    "\n",
    "    if \"lr\" in run_config[\"opter\"][\"opter_config\"] \\\n",
    "        and run_config[\"opter\"][\"opter_config\"][\"lr\"] == find_best_lr:\n",
    "        print(\"  > Finding best lr...\")\n",
    "        run_config[\"opter\"][\"opter_config\"][\"lr\"] = find_best_lr(\n",
    "            opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "            opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "            optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "            optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "            data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "            data_config=run_config[\"data\"][\"data_config\"],\n",
    "            # loss_fn=run_config[\"loss_fn\"],\n",
    "            n_iters=120,\n",
    "            n_tests=1,\n",
    "            consider_metric=\"loss\",\n",
    "            lrs_to_try=[0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0],\n",
    "        )\n",
    "        print(f\"  > Best lr: {run_config['opter']['opter_config']['lr']}\")\n",
    "\n",
    "    print(\"  > Running...\")\n",
    "    runs[run_name][\"log\"] = dict()\n",
    "    for i in range(n_test_runs):\n",
    "        print(f\"    > Run {i+1}/{n_test_runs}...\")\n",
    "\n",
    "        ### check if L2O has been meta-trained\n",
    "        assert not run_config[\"opter\"][\"opter_cls\"] == L2O or run_config[\"l2o_dict\"] is not None\n",
    "\n",
    "        curr_log = do_fit(\n",
    "            opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "            opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "            optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "            optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "            data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "            data_config=run_config[\"data\"][\"data_config\"],\n",
    "            n_iters=run_config[\"n_iters\"],\n",
    "            l2o_dict=run_config[\"l2o_dict\"],\n",
    "            in_meta_training=False,\n",
    "            additional_metrics=run_config[\"additional_metrics\"],\n",
    "        )[0]\n",
    "\n",
    "        for metric_name in curr_log.keys():\n",
    "            if metric_name not in runs[run_name][\"log\"]:\n",
    "                runs[run_name][\"log\"][metric_name] = []\n",
    "            runs[run_name][\"log\"][metric_name].append(curr_log[metric_name])\n",
    "\n",
    "    runs[run_name][\"config\"] = run_config\n",
    "\n",
    "    ### save results\n",
    "    torch.save(runs[run_name], save_to, pickle_module=dill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_win = 5\n",
    "plot_log(\n",
    "    runs,\n",
    "    only_metrics=[\"loss\", \"cos_sim(d, x.grad)\"],\n",
    "    # only_metrics=[\"loss\"],\n",
    "    # only_metrics=[\"cos_sim(d, x.grad)\"],\n",
    "    log_metrics=[\"loss\", \"l2_dist(x_tik*, x)\", \"l2_dist(x*, x)\"],\n",
    "    conv_win=conv_win,\n",
    "    min_max_y_config={\n",
    "        \"last_lr\": (0, 100),\n",
    "    },\n",
    "    save_to=os.path.join(\n",
    "        config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "        f\"loss_cos_sim__{dict_to_str(run_config['optee']['optee_config'])}__{conv_win}conv_{n_test_runs}runs_{test_run_iters}iters.png\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_publication_plt_settings(font_size=16, dpi=600, figsize=(5, 3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting config\n",
    "metric = \"loss\"\n",
    "show_max_iters = 500\n",
    "log_metric = True\n",
    "conv_window = 5\n",
    "\n",
    "### where to save the figure\n",
    "fig_dir = \"../results/publication/nn/src\"\n",
    "fig_name = f\"{metric}_{dict_to_str(run_config['optee']['optee_config'])}_{show_max_iters}iters.pdf\"\n",
    "if log_metric:\n",
    "    fig_name = f\"log_{fig_name}\"\n",
    "save_fig_to_path = os.path.join(fig_dir, fig_name)\n",
    "save_fig_to_path = None # don't save\n",
    "print(f\"Final destination: {save_fig_to_path if save_fig_to_path is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(\n",
    "    baselines={\n",
    "        k: r for k, r in runs.items() if \"L2O-CFGD\" not in k\n",
    "    },\n",
    "    l2os={\n",
    "        k: r for k, r in runs.items() if \"L2O-CFGD\" in k\n",
    "    },\n",
    "    metric=metric,\n",
    "    show_max_iters=show_max_iters,\n",
    "    log_metric=log_metric,\n",
    "    with_err_bars=True,\n",
    "    conv_window=conv_window,\n",
    "    save_fig_to_path=save_fig_to_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting config\n",
    "to_plot_label = r\"$c$\"\n",
    "max_components = 100\n",
    "to_plot_label_cleaned = ''.join(e for e in to_plot_label if e.isalnum())\n",
    "to_plot = np.stack(runs[\"L2O-CFGD\"][\"log\"][to_plot_label_cleaned])[0].squeeze().reshape(test_run_iters, -1)[:show_max_iters,:] # (n_iters, num_of_param_elems)\n",
    "component_idxs = np.random.choice(to_plot.shape[-1], size=max_components, replace=False)\n",
    "to_plot_sampled = to_plot[:, component_idxs]\n",
    "\n",
    "### where to save the figure\n",
    "fig_dir = \"../results/publication/quadratic/src\"\n",
    "fig_name = f\"{to_plot_label_cleaned}_{test_run_iters}iters.pdf\"\n",
    "save_fig_to_path = os.path.join(fig_dir, fig_name)\n",
    "save_fig_to_path = None # don't save\n",
    "print(f\"Final destination: {save_fig_to_path if save_fig_to_path is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_strategy(\n",
    "    to_plot=to_plot_sampled,\n",
    "    y_label=to_plot_label,\n",
    "    save_fig_to_path=save_fig_to_path,\n",
    "    mean_to_plot=to_plot.mean(-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple parameters\n",
    "- How do alphas, betas, cs and grads correlate with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_publication_plt_settings(font_size=15, dpi=600, figsize=(13, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.stack(runs[\"L2O-CFGD\"][\"log\"][\"alpha\"]).reshape(n_test_runs, test_run_iters, -1)\n",
    "betas = np.stack(runs[\"L2O-CFGD\"][\"log\"][\"beta\"]).reshape(n_test_runs, test_run_iters, -1)\n",
    "cs = np.stack(runs[\"L2O-CFGD\"][\"log\"][\"c\"]).reshape(n_test_runs, test_run_iters, -1)  # (n_test_runs, n_iters, D)\n",
    "grads = np.stack(runs[\"L2O-CFGD\"][\"log\"][\"grad\"]).reshape(n_test_runs, test_run_iters, -1)  # (n_test_runs, n_iters, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_components = 200\n",
    "component_idxs = np.random.choice(alphas.shape[-1], size=max_components, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run_idx = 0\n",
    "iters_to_show = [0, 1, 2, 5, 20]\n",
    "\n",
    "# fig = plt.figure(figsize=(14, 18), facecolor=\"white\")\n",
    "fig = plt.figure()\n",
    "# fig.suptitle(\"L2O + CFGD_ClosedForm\")\n",
    "ax_idx = 1\n",
    "\n",
    "for i, iter_idx in enumerate(iters_to_show):\n",
    "    ax = fig.add_subplot(len(iters_to_show), 3, ax_idx)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    \n",
    "    ### alphas\n",
    "    sns.scatterplot(\n",
    "        x=grads[test_run_idx, iter_idx, component_idxs],\n",
    "        y=alphas[test_run_idx, iter_idx, component_idxs],\n",
    "        ax=ax,\n",
    "    )\n",
    "    if i == len(iters_to_show) - 1:\n",
    "        ax.set_xlabel(r\"$\\partial_{x_j} f(x_j)$\")\n",
    "    ax.set_ylabel(r\"$\\alpha$\")\n",
    "    # ax.set_title(fr\"Iteration {iter_idx}\")\n",
    "\n",
    "    ### betas\n",
    "    ax = fig.add_subplot(len(iters_to_show), 3, ax_idx + 1)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    sns.scatterplot(\n",
    "        x=grads[test_run_idx, iter_idx, component_idxs],\n",
    "        y=betas[test_run_idx, iter_idx, component_idxs],\n",
    "        ax=ax,\n",
    "    )\n",
    "    if i == len(iters_to_show) - 1:\n",
    "        ax.set_xlabel(r\"$\\partial_{x_j} f(x_j)$\")\n",
    "    ax.set_ylabel(r\"$\\beta$\")\n",
    "    ax.set_title(fr\"Iteration {iter_idx}\", pad=10)\n",
    "    # ax.set_title(fr\"$\\beta$ (iter {iter_idx})\")\n",
    "\n",
    "    ### cs\n",
    "    ax = fig.add_subplot(len(iters_to_show), 3, ax_idx + 2)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    sns.scatterplot(\n",
    "        x=grads[test_run_idx, iter_idx, component_idxs],\n",
    "        y=cs[test_run_idx, iter_idx, component_idxs],\n",
    "        ax=ax,\n",
    "    )\n",
    "    if i == len(iters_to_show) - 1:\n",
    "        ax.set_xlabel(r\"$\\partial_{x_j} f(x_j)$\")\n",
    "    ax.set_ylabel(r\"$c$\")\n",
    "    # ax.set_title(fr\"$c$ (iter {iter_idx})\")\n",
    "\n",
    "    ax_idx += 3\n",
    "\n",
    "# fig.tight_layout(h_pad=1.5)\n",
    "# save_to = os.path.join(\n",
    "#     config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "#     f\"strategy_grad_alpha_beta_c_{test_d}d_{test_m}m_{n_test_runs}runs_{test_run_iters}iters.png\"\n",
    "# )\n",
    "# fig.savefig(save_to)\n",
    "plt.tight_layout(h_pad=2.2)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"../results/strategy.eps\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
