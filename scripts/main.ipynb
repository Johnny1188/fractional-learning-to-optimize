{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH='/home/jsobotka/Desktop/Dev/data'\n",
      "CKPT_PATH='/media/jsobotka/ext_ssd/fl2o/ckpt'\n",
      "DEVICE='cuda'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lovely_tensors as lt # can be removed\n",
    "import numpy as np\n",
    "import dill\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from fl2o.optimizee import MLPOptee, CustomParams\n",
    "from fl2o.optimizee_modules import MetaParameter\n",
    "from fl2o.optimizer import GD, FGD, AFOGD, CFGD, CFGD_ClosedForm\n",
    "from fl2o.l2o import L2O\n",
    "from fl2o.data import MNIST, CustomTask, generate_least_squares_task\n",
    "from fl2o.training import do_fit, find_best_lr, meta_train, get_optimal_lr\n",
    "from fl2o.utils import plot_log, plotter\n",
    "\n",
    "lt.monkey_patch() # can be removed\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "CKPT_PATH = os.getenv(\"CKPT_PATH\")\n",
    "DEVICE = os.getenv(\"DEVICE\", \"cpu\")\n",
    "\n",
    "print(f\"{DATA_PATH=}\\n{CKPT_PATH=}\\n{DEVICE=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load previous checkpoint (and skip meta-training of a new l2O optimizer)\n",
    "ckpt = torch.load(\n",
    "    os.path.join(\n",
    "        CKPT_PATH,\n",
    "        \"l2o\",\n",
    "        \"25-01_00-34__L2O__CFGD_ClosedForm\",\n",
    "        \"ckpt.pth\"\n",
    "    ),\n",
    "    map_location=torch.device(DEVICE),\n",
    "    pickle_module=dill,\n",
    ")\n",
    "l2o_dict = ckpt[\"l2o_dict\"]\n",
    "l2o_dict_best = ckpt[\"l2o_dict_best\"]\n",
    "log = ckpt[\"log\"]\n",
    "config = ckpt[\"config\"]\n",
    "print(json.dumps(config, indent=4, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min_{x} f(x) = \\frac{1}{2} ||W^T x - y||_2^2 \\\\\n",
    "    \\text{where } W \\in \\mathbb{R}^{d \\times m}, y \\in \\mathbb{R}^m\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"time\": datetime.now().strftime(\"%d-%m_%H-%M\"),\n",
    "}\n",
    "\n",
    "### data (task)\n",
    "config[\"data\"] = {\n",
    "    \"d\": 100,\n",
    "    \"m\": 100,\n",
    "    \"data_cls\": CustomTask,\n",
    "}\n",
    "config[\"data\"][\"data_config\"] = {\n",
    "    \"task\": generate_least_squares_task,\n",
    "    \"task_config\": {\n",
    "        \"d\": config[\"data\"][\"d\"],\n",
    "        \"m\": config[\"data\"][\"m\"],\n",
    "        \"verbose\": False,\n",
    "        \"device\": DEVICE,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "### optimizee\n",
    "config[\"optee\"] = {\n",
    "    \"optee_cls\": CustomParams,\n",
    "    \"optee_config\": {\n",
    "        \"dim\": (1, config[\"data\"][\"d\"]),\n",
    "        \"init_params\": \"randn\",\n",
    "        # \"param_func\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "### optimizer\n",
    "config[\"opter\"] = {\n",
    "    \"opter_cls\": L2O,\n",
    "    \"opter_config\": {\n",
    "        \"in_dim\": 3, # len(in_features) + 1\n",
    "        \"out_dim\": 3,\n",
    "        \"hidden_sz\": 40,\n",
    "        \"in_features\": (\"grad\", \"iter_num_enc\"),\n",
    "        \"base_opter_cls\": CFGD_ClosedForm,\n",
    "        \"base_opter_config\": {\n",
    "            \"lr\": get_optimal_lr,\n",
    "            \"gamma\": None,\n",
    "            \"c\": None,\n",
    "            \"version\": \"NA\",\n",
    "            \"device\": DEVICE,\n",
    "        },\n",
    "        \"params_to_optimize\": {\n",
    "            # \"gamma\": {\n",
    "            #     \"idx\": 0,\n",
    "            #     \"act_fns\": (\"identity\", \"diag\"),\n",
    "            # },\n",
    "            # \"gamma\": {\n",
    "            #     \"idx\": 0,\n",
    "            #     \"act_fns\": (\"alpha_to_gamma\", \"diag\"),\n",
    "            #     \"beta\": 0.,\n",
    "            # },\n",
    "            \"gamma\": {\n",
    "                \"idx\": (0, 1),\n",
    "                \"act_fns\": (\"alpha_beta_to_gamma\", \"diag\"),\n",
    "            },\n",
    "            \"c\": {\n",
    "                \"idx\": 2,\n",
    "                \"act_fns\": (\"identity\",),\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "config[\"meta_training_config\"] = {\n",
    "    \"meta_opter_cls\": optim.Adam,\n",
    "    \"meta_opter_config\": {\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "    \"n_runs\": 2000,\n",
    "    \"unroll\": 20,\n",
    "    \"loggers\": [\n",
    "        {\n",
    "            \"every_nth_run\": 20,\n",
    "            \"logger_fn\": partial(plotter, to_plot=\"gamma\"),\n",
    "        },\n",
    "        {\n",
    "            \"every_nth_run\": 20,\n",
    "            \"logger_fn\": partial(plotter, to_plot=\"c\"),\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "### other\n",
    "config.update({\n",
    "    \"n_iters\": 800,\n",
    "    \"l2o_dict\": None,\n",
    "    \"additional_metrics\": {\n",
    "        \"gamma\": lambda opter, **kwargs: \\\n",
    "            # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "            opter.base_opter.param_groups[0][\"gamma\"].mean().item() \\\n",
    "            if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"gamma\", None),\n",
    "        \"c\": lambda opter, **kwargs: \\\n",
    "            # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "            opter.base_opter.param_groups[0][\"c\"].mean().item() \\\n",
    "            if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"c\", None),\n",
    "        # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "        #     torch.norm(task[\"x_tik_solution\"](gamma=1., c=1) - optee.params.detach(), p=2).item(),\n",
    "        # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "        #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "    },\n",
    "    \"ckpt_config\": {\n",
    "        \"ckpt_every_nth_run\": 50,\n",
    "        \"ckpt_dir\": os.path.join(\n",
    "            CKPT_PATH,\n",
    "            \"l2o\",\n",
    "            config[\"time\"] + \"__\"\\\n",
    "                + config[\"opter\"][\"opter_cls\"].__name__ + \"__\"\\\n",
    "                + config[\"opter\"][\"opter_config\"][\"base_opter_cls\"].__name__,\n",
    "        ),\n",
    "    },\n",
    "    \"device\": DEVICE,\n",
    "    \"seed\": 0,\n",
    "})\n",
    "config[\"ckpt_config\"][\"ckpt_dir_meta_training\"] = os.path.join(\n",
    "    config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "    \"meta_training\",\n",
    ")\n",
    "config[\"ckpt_config\"][\"ckpt_dir_meta_testing\"] = os.path.join(\n",
    "    config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "    \"meta_testing\",\n",
    ")\n",
    "\n",
    "### make dirs\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir\"], exist_ok=True)\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir_meta_training\"], exist_ok=True)\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir_meta_testing\"], exist_ok=True)\n",
    "\n",
    "### save config\n",
    "with open(os.path.join(config[\"ckpt_config\"][\"ckpt_dir\"], \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4, default=str)\n",
    "\n",
    "print(f\"Path to checkpoints: {config['ckpt_config']['ckpt_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta train\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "l2o_dict, l2o_dict_best, log = meta_train(\n",
    "    opter_cls=config[\"opter\"][\"opter_cls\"],\n",
    "    opter_config=config[\"opter\"][\"opter_config\"],\n",
    "    optee_cls=config[\"optee\"][\"optee_cls\"],\n",
    "    optee_config=config[\"optee\"][\"optee_config\"],\n",
    "    data_cls=config[\"data\"][\"data_cls\"],\n",
    "    data_config=config[\"data\"][\"data_config\"],\n",
    "    n_iters=config[\"n_iters\"],\n",
    "    meta_opter_cls=config[\"meta_training_config\"][\"meta_opter_cls\"],\n",
    "    meta_opter_config=config[\"meta_training_config\"][\"meta_opter_config\"],\n",
    "    n_runs=config[\"meta_training_config\"][\"n_runs\"],\n",
    "    unroll=config[\"meta_training_config\"][\"unroll\"],\n",
    "    additional_metrics=config[\"additional_metrics\"],\n",
    "    loggers=config[\"meta_training_config\"][\"loggers\"],\n",
    "    ckpt_config=config[\"ckpt_config\"],\n",
    "    \n",
    "    ### keep meta-training\n",
    "    # l2o_dict=l2o_dict,\n",
    "    # l2o_dict_best=l2o_dict_best,\n",
    "    # log=log,\n",
    ")\n",
    "\n",
    "### save checkpoint\n",
    "torch.save({\n",
    "    \"l2o_dict\": l2o_dict,\n",
    "    \"l2o_dict_best\": l2o_dict_best,\n",
    "    \"log\": log,\n",
    "    \"config\": config,\n",
    "}, os.path.join(config[\"ckpt_config\"][\"ckpt_dir\"], \"ckpt.pth\"), pickle_module=dill)\n",
    "\n",
    "plt.plot(log[\"loss_sum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta-testing config\n",
    "test_d, test_m = 100, 100\n",
    "n_test_runs = 1\n",
    "test_run_iters = 3000\n",
    "test_runs_seed = config[\"seed\"]\n",
    "# test_runs_seed = 11\n",
    "\n",
    "runs = dict()\n",
    "\n",
    "runs[\"GD\"] = {\n",
    "    \"update_config\": {\n",
    "        \"data\": {\n",
    "            \"d\": test_d,\n",
    "            \"m\": test_m,\n",
    "            \"data_cls\": CustomTask,\n",
    "            \"data_config\": {\n",
    "                \"task\": generate_least_squares_task,\n",
    "                \"task_config\": {\n",
    "                    \"d\": test_d,\n",
    "                    \"m\": test_m,\n",
    "                    \"verbose\": False,\n",
    "                    \"device\": DEVICE,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"optee\": {\n",
    "            \"optee_cls\": CustomParams,\n",
    "            \"optee_config\": {\n",
    "                \"dim\": (1, test_d),\n",
    "                \"init_params\": \"randn\",\n",
    "                # \"param_func\": None,\n",
    "            },\n",
    "        },\n",
    "        \"opter\": {\n",
    "            \"opter_cls\": GD,\n",
    "            \"opter_config\": {\n",
    "                \"lr\": get_optimal_lr,\n",
    "                \"device\": DEVICE,\n",
    "            },\n",
    "        },\n",
    "        \"additional_metrics\": {\n",
    "            # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "            #     torch.norm(task[\"x_tik_solution\"](gamma=0., c=1) - optee.params.detach(), p=2).item(),\n",
    "            # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "            #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "            # \"x\": lambda y_hat, y, optee: optee.params.detach().cpu().numpy(),\n",
    "            \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "                torch.cosine_similarity(\n",
    "                    opter.param_groups[0][\"last_update\"].flatten(),\n",
    "                    opter.param_groups[0][\"last_grad\"].flatten(),\n",
    "                    dim=0\n",
    "                ).item(),\n",
    "            \"last_lr\": lambda opter, **kwargs: \\\n",
    "                opter.param_groups[0][\"last_lr\"].item() if type(opter.param_groups[0][\"last_lr\"]) == torch.Tensor else opter.param_groups[0][\"last_lr\"],\n",
    "        },\n",
    "        \"n_iters\": test_run_iters,\n",
    "    },\n",
    "    \"plot_config\": {\n",
    "        \"color\": \"black\",\n",
    "        \"linestyle\": \"dashed\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# for gamma in [-0.23, 0.01, 0.05, 0.5, 1.]:\n",
    "#     runs[r\"NA-CFGD_ClosedForm, $\\gamma$=\" + str(gamma)] = {\n",
    "#         \"update_config\": {\n",
    "#             \"data\": {\n",
    "#                 \"d\": test_d,\n",
    "#                 \"m\": test_m,\n",
    "#                 \"data_cls\": CustomTask,\n",
    "#                 \"data_config\": {\n",
    "#                     \"task\": generate_least_squares_task,\n",
    "#                     \"task_config\": {\n",
    "#                         \"d\": test_d,\n",
    "#                         \"m\": test_m,\n",
    "#                         \"verbose\": False,\n",
    "#                         \"device\": DEVICE,\n",
    "#                     },\n",
    "#                 },\n",
    "#             },\n",
    "#             \"optee\": {\n",
    "#                 \"optee_cls\": CustomParams,\n",
    "#                 \"optee_config\": {\n",
    "#                     \"dim\": (1, test_d),\n",
    "#                     \"init_params\": \"randn\",\n",
    "#                     # \"param_func\": None,\n",
    "#                 },\n",
    "#             },\n",
    "#             \"opter\": {\n",
    "#                 \"opter_cls\": CFGD_ClosedForm,\n",
    "#                 \"opter_config\": {\n",
    "#                     \"lr\": get_optimal_lr,\n",
    "#                     \"gamma\": gamma,\n",
    "#                     \"c\": 1.,\n",
    "#                     \"device\": DEVICE,\n",
    "#                 },\n",
    "#             },\n",
    "#             \"additional_metrics\": {\n",
    "#                 # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "#                 #     torch.norm(task[\"x_tik_solution\"](gamma=gamma, c=1) - optee.params.detach(), p=2).item(),\n",
    "#                 # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "#                 #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "#                 # \"x\": lambda y_hat, y, optee: optee.params.detach().cpu().numpy(),\n",
    "#                 \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "#                     torch.cosine_similarity(\n",
    "#                         opter.state[0][\"last_update\"].flatten(),\n",
    "#                         opter.state[0][\"last_grad\"].flatten(),\n",
    "#                         dim=0\n",
    "#                     ).item(),\n",
    "#                 \"last_lr\": lambda opter, **kwargs: \\\n",
    "#                     opter.state[0][\"last_lr\"].item() if type(opter.state[0][\"last_lr\"]) == torch.Tensor else opter.state[0][\"last_lr\"],\n",
    "#             },\n",
    "#             \"n_iters\": test_run_iters,\n",
    "#         },\n",
    "#         \"plot_config\": {\n",
    "#             \"linestyle\": \"dashed\",\n",
    "#         },\n",
    "#     }\n",
    "\n",
    "runs[\"L2O + CFGD_ClosedForm\"] = {\n",
    "    \"update_config\": {\n",
    "        \"data\": {\n",
    "            \"d\": test_d,\n",
    "            \"m\": test_m,\n",
    "            \"data_cls\": CustomTask,\n",
    "            \"data_config\": {\n",
    "                \"task\": generate_least_squares_task,\n",
    "                \"task_config\": {\n",
    "                    \"d\": test_d,\n",
    "                    \"m\": test_m,\n",
    "                    \"verbose\": False,\n",
    "                    \"device\": DEVICE,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"optee\": {\n",
    "            \"optee_cls\": CustomParams,\n",
    "            \"optee_config\": {\n",
    "                \"dim\": (1, test_d),\n",
    "                \"init_params\": \"randn\",\n",
    "                # \"param_func\": None,\n",
    "            },\n",
    "        },\n",
    "        \"additional_metrics\": {\n",
    "            \"gamma\": lambda opter, **kwargs: \\\n",
    "                # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "                opter.base_opter.param_groups[0][\"gamma\"].detach().cpu().numpy() \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"gamma\", None),\n",
    "            \"c\": lambda opter, **kwargs: \\\n",
    "                # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "                opter.base_opter.param_groups[0][\"c\"].detach().cpu().numpy() \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"c\", None),\n",
    "            \"alpha\": lambda opter, **kwargs: \\\n",
    "                # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "                opter.base_opter.param_groups[0][\"alpha\"] \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"alpha\", None),\n",
    "            \"beta\": lambda opter, **kwargs: \\\n",
    "                # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "                opter.base_opter.param_groups[0][\"beta\"] \\\n",
    "                if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"beta\", None),\n",
    "            # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "            #     torch.norm(task[\"x_tik_solution\"](gamma=0., c=1) - optee.params.detach(), p=2).item(),\n",
    "            # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "            #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "            # \"x\": lambda y_hat, y, optee: optee.params.detach().cpu().numpy(),\n",
    "        },\n",
    "        \"l2o_dict\": l2o_dict,\n",
    "        # \"l2o_dict\": l2o_dict_best[\"best_l2o_dict\"],\n",
    "        \"n_iters\": test_run_iters,\n",
    "    },\n",
    "    \"plot_config\": {\n",
    "        \"color\": \"orange\",\n",
    "        \"linewidth\": \"3\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# runs[\"CFGD\"] = {\n",
    "#     \"update_config\": {\n",
    "#         \"data\": {\n",
    "#             \"d\": test_d,\n",
    "#             \"m\": test_m,\n",
    "#             \"data_cls\": CustomTask,\n",
    "#             \"data_config\": {\n",
    "#                 \"task\": generate_least_squares_task,\n",
    "#                 \"task_config\": {\n",
    "#                     \"d\": test_d,\n",
    "#                     \"m\": test_m,\n",
    "#                     \"verbose\": False,\n",
    "#                     \"device\": DEVICE,\n",
    "#                 },\n",
    "#             },\n",
    "#         },\n",
    "#         \"optee\": {\n",
    "#             \"optee_cls\": CustomParams,\n",
    "#             \"optee_config\": {\n",
    "#                 \"dim\": (1, test_d),\n",
    "#                 \"init_params\": \"randn\",\n",
    "#                 # \"param_func\": None,\n",
    "#             },\n",
    "#         },\n",
    "#         \"opter\": {\n",
    "#             \"opter_cls\": CFGD,\n",
    "#             \"opter_config\": {\n",
    "#                 \"lr\": get_optimal_lr,\n",
    "#                 # \"lr\": 0.1,\n",
    "#                 \"alpha\": 0.9,\n",
    "#                 # \"beta\": 30.23,\n",
    "#                 \"beta\": 0.,\n",
    "#                 \"c\": 1,\n",
    "#                 \"s\": 3,\n",
    "#                 \"device\": DEVICE,\n",
    "#             },\n",
    "#         },\n",
    "#         \"additional_metrics\": {\n",
    "#             # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "#             #     torch.norm(task[\"x_tik_solution\"](gamma=0., c=1) - optee.params.detach(), p=2).item(),\n",
    "#             # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "#             #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "#             # \"x\": lambda y_hat, y, optee: optee.params.detach().cpu().numpy(),\n",
    "#             # \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "#             #     torch.cosine_similarity(\n",
    "#             #         opter.state[0][\"last_update\"].flatten(),\n",
    "#             #         opter.state[0][\"last_grad\"].flatten(),\n",
    "#             #         dim=0\n",
    "#             #     ).item(),\n",
    "#             # \"last_lr\": lambda opter, **kwargs: \\\n",
    "#             #     opter.state[0][\"last_lr\"].item() if type(opter.state[0][\"last_lr\"]) == torch.Tensor else opter.state[0][\"last_lr\"],\n",
    "#         },\n",
    "#         \"n_iters\": test_run_iters,\n",
    "#     },\n",
    "#     \"plot_config\": {\n",
    "#         \"color\": \"orange\",\n",
    "#         \"linewidth\": \"3\",\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run all\n",
    "for run_name in runs.keys():\n",
    "    if \"log\" in runs[run_name]:\n",
    "        continue # already run\n",
    "    run_config = copy.deepcopy(config)\n",
    "    if \"update_config\" in runs[run_name] and runs[run_name][\"update_config\"] is not None:\n",
    "        run_config.update(runs[run_name][\"update_config\"])\n",
    "    print(f\"{run_name}:\")\n",
    "    \n",
    "    torch.manual_seed(test_runs_seed)\n",
    "    np.random.seed(test_runs_seed)\n",
    "\n",
    "    if \"lr\" in run_config[\"opter\"][\"opter_config\"] \\\n",
    "        and run_config[\"opter\"][\"opter_config\"][\"lr\"] == find_best_lr:\n",
    "        print(\"  > Finding best lr...\")\n",
    "        run_config[\"opter\"][\"opter_config\"][\"lr\"] = find_best_lr(\n",
    "            opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "            opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "            optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "            optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "            data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "            data_config=run_config[\"data\"][\"data_config\"],\n",
    "            # loss_fn=run_config[\"loss_fn\"],\n",
    "            n_iters=120,\n",
    "            n_tests=1,\n",
    "            consider_metric=\"loss\",\n",
    "            lrs_to_try=[0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0],\n",
    "        )\n",
    "        print(f\"  > Best lr: {run_config['opter']['opter_config']['lr']}\")\n",
    "\n",
    "    print(\"  > Running...\")\n",
    "    runs[run_name][\"log\"] = dict()\n",
    "    for i in range(n_test_runs):\n",
    "        print(f\"    > Run {i+1}/{n_test_runs}...\")\n",
    "\n",
    "        ### check if L2O has been meta-trained\n",
    "        assert not run_config[\"opter\"][\"opter_cls\"] == L2O or run_config[\"l2o_dict\"] is not None\n",
    "\n",
    "        curr_log = do_fit(\n",
    "            opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "            opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "            optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "            optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "            data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "            data_config=run_config[\"data\"][\"data_config\"],\n",
    "            n_iters=run_config[\"n_iters\"],\n",
    "            l2o_dict=run_config[\"l2o_dict\"],\n",
    "            in_meta_training=False,\n",
    "            additional_metrics=run_config[\"additional_metrics\"],\n",
    "        )[0]\n",
    "\n",
    "        for metric_name in curr_log.keys():\n",
    "            if metric_name not in runs[run_name][\"log\"]:\n",
    "                runs[run_name][\"log\"][metric_name] = []\n",
    "            runs[run_name][\"log\"][metric_name].append(curr_log[metric_name])\n",
    "\n",
    "    runs[run_name][\"config\"] = run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log(\n",
    "    runs,\n",
    "    only_metrics=[\"loss\"],\n",
    "    log_metrics=[\"loss\", \"l2_dist(x_tik*, x)\", \"l2_dist(x*, x)\"],\n",
    "    conv_win=1,\n",
    "    min_max_y_config={\n",
    "        \"last_lr\": (0, 100),\n",
    "    },\n",
    "    save_to=os.path.join(\n",
    "        config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "        f\"loss_l2o_best_dict_{test_d}d_{test_m}m_{n_test_runs}runs_{test_run_iters}iters.png\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 30)\n",
    "w = torch.randn(3, 10, 30, requires_grad=True)\n",
    "b = torch.randn(3, 10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.nn.Linear(30, 10, bias=True)\n",
    "l.weight.data = x\n",
    "l.weight.requires_grad_(False)\n",
    "l.bias.data = b.T\n",
    "l.bias.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.nn.Linear(40, 10, bias=False)\n",
    "l.weight.data[:, :x.shape[1]] = x\n",
    "l.weight.data[:, x.shape[1]:] = 0.\n",
    "for b_idx in range(b.shape[-1]):\n",
    "    l.weight.data[b_idx, x.shape[1] + b_idx] = 1.\n",
    "l.weight.requires_grad_(False)\n",
    "# l.bias.data = b\n",
    "# l.bias.requires_grad_(True)\n",
    "\n",
    "### grad grad wrt w and b\n",
    "inp = torch.cat([w, b.unsqueeze(1).expand(-1, w.shape[1], -1)], dim=2)\n",
    "w_grad, b_grad = torch.autograd.grad(l(inp).sum(), (w, b))\n",
    "b_grad /= w.shape[1]  # for expanding b\n",
    "# l(inp).sum().backward()\n",
    "# w_grad = w.grad\n",
    "# b_grad = b.grad / w.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optee = MLPOptee().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = MNIST(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = d.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fl2o.optimizee import get_mlp_optee_mirror\n",
    "\n",
    "model = get_mlp_optee_mirror(optee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd_hacks\n",
    "\n",
    "autograd_hacks.add_hooks(model)\n",
    "autograd_hacks.backprop_hess(model(task), hess_type='CrossEntropy')\n",
    "autograd_hacks.compute_hess(model)\n",
    "for param in model.parameters():\n",
    "    print(param.hess)  # print Hessian of param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_idx, (n, p) in enumerate(optee.all_named_parameters()):\n",
    "    if p_idx == 0:\n",
    "        continue\n",
    "    deriv_eval_points = torch.rand_like(p.data)\n",
    "    pp = p.data.detach().clone().view(-1).unsqueeze(0).repeat(p.shape.numel(), 1) # (param_size, param_size)\n",
    "    pp[torch.arange(p.shape.numel()), torch.arange(p.shape.numel())] = deriv_eval_points.view(-1) # (param_size, param_size)\n",
    "    \n",
    "    ### get the hessian\n",
    "    forward_w_params = pp.view(p.shape.numel(), *p.shape)\n",
    "    y_hat = optee.forward_w_params(task=task, params=forward_w_params, params_for=n)\n",
    "    loss = task[\"loss_fn\"](y_hat=y_hat)\n",
    "    fo_out = torch.autograd.grad(\n",
    "        outputs=loss,\n",
    "        inputs=forward_w_params,\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "    ### get he hessian\n",
    "    he_out = torch.autograd.grad(\n",
    "        outputs=fo_out,\n",
    "        inputs=forward_w_params,\n",
    "        # grad_outputs=torch.ones_like(fo_out),\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_w_params = [p.detach().clone().view(-1) for p in optee.parameters()]\n",
    "forward_w_params = torch.stack(forward_w_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = optee.forward_w_params(task=task, params=forward_w_params)\n",
    "loss = task[\"loss_fn\"](y_hat=y_hat)\n",
    "fo_out = torch.autograd.grad(\n",
    "    outputs=loss,\n",
    "    inputs=forward_w_params,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gammas = np.stack(runs[\"L2O + CFGD_ClosedForm\"][\"log\"][\"gamma\"])\n",
    "# gammas = gammas.T.diagonal().transpose(1, 0, 2)  # (n_test_runs, n_iters, D)\n",
    "\n",
    "alphas = np.stack(runs[\"L2O + CFGD_ClosedForm\"][\"log\"][\"alpha\"])\n",
    "# alphas = alphas.T.diagonal().transpose(1, 0, 2)  # (n_test_runs, n_iters, D)\n",
    "\n",
    "betas = np.stack(runs[\"L2O + CFGD_ClosedForm\"][\"log\"][\"beta\"])\n",
    "# betas = betas.T.diagonal().transpose(1, 0, 2)  # (n_test_runs, n_iters, D)\n",
    "\n",
    "cs = np.stack(runs[\"L2O + CFGD_ClosedForm\"][\"log\"][\"c\"])  # (n_test_runs, n_iters, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run_idx = 0\n",
    "to_plot_name = \"c\"\n",
    "latex_expr = \"c\"\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6), facecolor=\"w\")\n",
    "fig.suptitle(rf\"L2O-CFGD: ${latex_expr}$\", fontsize=16)\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "if to_plot_name == \"alpha\":\n",
    "    to_plot = alphas\n",
    "elif to_plot_name == \"beta\":\n",
    "    to_plot = betas\n",
    "elif to_plot_name == \"gamma\":\n",
    "    to_plot = gammas\n",
    "elif to_plot_name == \"c\":\n",
    "    to_plot = cs\n",
    "else:\n",
    "    raise ValueError(f\"Unknown to_plot: {to_plot_name}\")\n",
    "\n",
    "plt.plot(to_plot[test_run_idx].squeeze(), alpha=0.1, color=\"grey\")\n",
    "plt.plot(to_plot[test_run_idx].squeeze().mean(-1), color=\"orange\", linewidth=3)\n",
    "ax.set_xlabel(\"Iteration\", fontsize=13)\n",
    "ax.set_ylabel(rf\"${latex_expr}$\", fontsize=13)\n",
    "\n",
    "### share y-axis with left plot\n",
    "ax = fig.add_subplot(122, sharey=ax)\n",
    "plt.plot(to_plot[test_run_idx].squeeze()[:,:3])\n",
    "ax.set_xlabel(\"Iteration\", fontsize=13)\n",
    "ax.set_ylabel(rf\"${latex_expr}$\", fontsize=13)\n",
    "ax.legend([\"Parameter #1\", \"Parameter #2\", \"Parameter #3\"])\n",
    "\n",
    "plt.tight_layout(h_pad=2.5)\n",
    "save_to = os.path.join(\n",
    "    config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "    f\"strategy_{to_plot_name}_{test_d}d_{test_m}m_{n_test_runs}runs_{test_run_iters}iters.png\"\n",
    ")\n",
    "fig.savefig(save_to)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run_idx = 0\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "fig.suptitle(r\"L2O-CFGD: $\\gamma$\", fontsize=16)\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "plt.plot(gammas[test_run_idx], alpha=0.1, color=\"grey\")\n",
    "plt.plot(gammas[test_run_idx].mean(-1), color=\"orange\", linewidth=3)\n",
    "ax.set_xlabel(\"Iteration\", fontsize=13)\n",
    "ax.set_ylabel(r\"$\\gamma$\", fontsize=13)\n",
    "\n",
    "### share y-axis with left plot\n",
    "ax = fig.add_subplot(122, sharey=ax)\n",
    "plt.plot(gammas[test_run_idx,:,:3])\n",
    "ax.set_xlabel(\"Iteration\", fontsize=13)\n",
    "ax.set_ylabel(r\"$\\gamma$\", fontsize=13)\n",
    "ax.legend([\"Parameter #1\", \"Parameter #2\", \"Parameter #3\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run_idx = 0\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "fig.suptitle(r\"L2O-CFGD: $c$\", fontsize=16)\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "plt.plot(cs[test_run_idx], alpha=0.1, color=\"grey\")\n",
    "plt.plot(cs[test_run_idx].mean(-1), color=\"orange\", linewidth=3)\n",
    "ax.set_xlabel(\"Iteration\", fontsize=13)\n",
    "ax.set_ylabel(r\"$c$\", fontsize=13)\n",
    "\n",
    "### share y-axis with left plot\n",
    "ax = fig.add_subplot(122, sharey=ax)\n",
    "plt.plot(cs[test_run_idx,:,:3])\n",
    "ax.set_xlabel(\"Iteration\", fontsize=13)\n",
    "ax.set_ylabel(r\"$c$\", fontsize=13)\n",
    "ax.legend([\"Parameter #1\", \"Parameter #2\", \"Parameter #3\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Objective Function (Figure 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quadratic objective function\n",
    "    min_x f(x, y) = 10 * x^2 + y^2\n",
    "\"\"\"\n",
    "def task_gen(device=\"cpu\"):\n",
    "    ### Least squares problem 1/2 x^T A x + b^T x\n",
    "    A = torch.tensor([[10., 0.], [0., 1.]], device=device)\n",
    "    b = torch.tensor([[0., 0.]], device=device).T\n",
    "    x_solution = torch.tensor([0., 0.], device=device)\n",
    "\n",
    "    loss_fn = lambda y_hat: 0.5 * y_hat @ A @ y_hat.T + b.T @ y_hat.T\n",
    "\n",
    "    return {\n",
    "        \"A\": A,\n",
    "        \"b\": b,\n",
    "        \"loss_fn\": loss_fn,\n",
    "        \"x_solution\": x_solution,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"optee\": {\n",
    "        \"optee_cls\": CustomParams,\n",
    "        \"optee_config\": {\n",
    "            \"dim\": (1, 2),\n",
    "            \"init_params\": torch.tensor([[1., -10.]], device=DEVICE),\n",
    "            \"param_func\": None,\n",
    "        },\n",
    "    },\n",
    "    \"opter\": {\n",
    "        \"opter_cls\": CFGD_ClosedForm,\n",
    "        \"opter_config\": {\n",
    "            \"lr\": get_optimal_lr,\n",
    "            \"gamma\": -1.,\n",
    "            \"c\": None,\n",
    "            \"version\": \"AT\",\n",
    "            \"init_points\": [torch.tensor([-1., -1.], device=DEVICE)],\n",
    "            \"device\": DEVICE,\n",
    "        },\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"data_cls\": CustomTask,\n",
    "        \"data_config\": {\n",
    "            \"task\": partial(task_gen, verbose=False, device=DEVICE),\n",
    "            \"device\": DEVICE,\n",
    "        },\n",
    "    },\n",
    "    \"n_iters\": 50,\n",
    "    \"l2o_dict\": None,\n",
    "    \"additional_metrics\": {\n",
    "        \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "            torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "    },\n",
    "    \"device\": DEVICE,\n",
    "    \"seed\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### runs config\n",
    "runs = dict()\n",
    "\n",
    "runs[\"GD\"] = {\n",
    "    \"update_config\": {\n",
    "        \"opter\": {\n",
    "            \"opter_cls\": GD,\n",
    "            \"opter_config\": {\n",
    "                \"lr\": get_optimal_lr,\n",
    "                \"device\": DEVICE,\n",
    "            },\n",
    "        },\n",
    "        \"additional_metrics\": {\n",
    "            \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "                torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "            \"x\": lambda optee, **kwargs: optee.params.detach().cpu().numpy(),\n",
    "        },\n",
    "    },\n",
    "    \"plot_config\": {\n",
    "        \"color\": \"black\",\n",
    "        \"linestyle\": \"dashed\",\n",
    "    },\n",
    "}\n",
    "\n",
    "for gamma in [-1., 0.01, 0.05, 0.25, 0.5, 1., 10.]:\n",
    "    runs[r\"CFGD_ClosedForm, $\\gamma$=\" + str(gamma)] = {\n",
    "        \"update_config\": {\n",
    "            \"opter\": {\n",
    "                \"opter_cls\": CFGD_ClosedForm,\n",
    "                \"opter_config\": {\n",
    "                    \"lr\": get_optimal_lr,\n",
    "                    \"gamma\": gamma,\n",
    "                    \"c\": None,\n",
    "                    \"version\": \"AT\",\n",
    "                    \"init_points\": [torch.tensor([-1., -1.], device=DEVICE)],\n",
    "                    \"device\": DEVICE,\n",
    "                },\n",
    "            },\n",
    "            \"additional_metrics\": {\n",
    "                \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "                    torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "                \"x\": lambda optee, **kwargs: optee.params.detach().cpu().numpy(),\n",
    "            },\n",
    "        },\n",
    "        # \"plot_config\": {\n",
    "        #     \"linestyle\": \"dashed\",\n",
    "        # },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run all\n",
    "for run_name in runs.keys():\n",
    "    if \"log\" in runs[run_name]:\n",
    "        continue # already run\n",
    "    run_config = copy.deepcopy(config)\n",
    "    if \"update_config\" in runs[run_name] and runs[run_name][\"update_config\"] is not None:\n",
    "        run_config.update(runs[run_name][\"update_config\"])\n",
    "    print(f\"{run_name}:\")\n",
    "    \n",
    "    torch.manual_seed(config[\"seed\"])\n",
    "    np.random.seed(config[\"seed\"])\n",
    "\n",
    "    if \"lr\" in run_config[\"opter\"][\"opter_config\"] \\\n",
    "        and run_config[\"opter\"][\"opter_config\"][\"lr\"] == find_best_lr:\n",
    "        print(\"  > Finding best lr...\")\n",
    "        run_config[\"opter\"][\"opter_config\"][\"lr\"] = find_best_lr(\n",
    "            opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "            opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "            optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "            optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "            data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "            data_config=run_config[\"data\"][\"data_config\"],\n",
    "            # loss_fn=run_config[\"loss_fn\"],\n",
    "            n_iters=120,\n",
    "            n_tests=1,\n",
    "            consider_metric=\"loss\",\n",
    "            lrs_to_try=[0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0],\n",
    "        )\n",
    "        print(f\"  > Best lr: {run_config['opter']['opter_config']['lr']}\")\n",
    "\n",
    "    print(\"  > Running...\")\n",
    "    runs[run_name][\"log\"], _, _ = do_fit(\n",
    "        opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "        opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "        optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "        optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "        data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "        data_config=run_config[\"data\"][\"data_config\"],\n",
    "        n_iters=run_config[\"n_iters\"],\n",
    "        l2o_dict=run_config[\"l2o_dict\"],\n",
    "        in_meta_training=False,\n",
    "        additional_metrics=run_config[\"additional_metrics\"],\n",
    "    )\n",
    "    runs[run_name][\"config\"] = run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log(\n",
    "    runs,\n",
    "    only_metrics=[\"loss\", \"l2_dist(x*, x)\", \"time\"],\n",
    "    log_metrics=[\"loss\", \"l2_dist(x_tik*, x)\", \"l2_dist(x*, x)\"],\n",
    "    conv_win=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x, y):\n",
    "    return 10 * x**2 + y**2\n",
    "\n",
    "def plot_optimizer_steps(runs, num_steps=10, only_opters=None, starting_points=None):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Plot the contour of the objective function\n",
    "    x_range = np.linspace(-1.5, 1.5, 100)\n",
    "    y_range = np.linspace(-10.5, 2, 100)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = objective_function(X, Y)\n",
    "    contour = plt.contour(Y, X, Z, levels=20, cmap='viridis')\n",
    "    plt.colorbar(contour, label='Objective Function Value')\n",
    "\n",
    "    for optimizer, data in runs.items():\n",
    "        if only_opters is not None and optimizer not in only_opters:\n",
    "            continue\n",
    "\n",
    "\n",
    "        x_values = [x.flatten() for x in data[\"log\"][\"x\"][:num_steps]]\n",
    "        if starting_points is not None:\n",
    "            x_values = starting_points + x_values\n",
    "        x_values = np.stack(x_values)\n",
    "\n",
    "        plt.plot(x_values[:, 1], x_values[:, 0], label=optimizer, marker='o', markersize=8, linewidth=3)\n",
    "\n",
    "    plt.title('Optimizer Steps')\n",
    "    plt.xlabel('y')\n",
    "    plt.ylabel('x')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'runs' with your actual dictionary\n",
    "# For demonstration, I'm using a simplified structure\n",
    "# runs = {\n",
    "#     \"GD\": {\"log\": {\"x\": [torch.randn(1, 2) for _ in range(20)]}},\n",
    "#     \"Adam\": {\"log\": {\"x\": [torch.randn(1, 2) for _ in range(20)]}},\n",
    "#     # Add more optimizers as needed\n",
    "# }\n",
    "\n",
    "plot_optimizer_steps(\n",
    "    runs,\n",
    "    only_opters=[\n",
    "        \"GD\",\n",
    "        \"CFGD_ClosedForm, $\\\\gamma$=-1.0\",\n",
    "        \"CFGD_ClosedForm, $\\\\gamma$=0.5\",\n",
    "        \"CFGD_ClosedForm, $\\\\gamma$=10.0\"\n",
    "    ],\n",
    "    num_steps=5,\n",
    "    starting_points=[[1., -10.]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to checkpoints: /media/jsobotka/ext_ssd/fl2o/ckpt/l2o/30-01_16-30__L2O__CFGD\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"time\": datetime.now().strftime(\"%d-%m_%H-%M\"),\n",
    "}\n",
    "\n",
    "### data (task)\n",
    "config[\"data\"] = {\n",
    "    \"data_cls\": MNIST,\n",
    "}\n",
    "config[\"data\"][\"data_config\"] = {\n",
    "    \"device\": DEVICE,\n",
    "    \"preload\": True,\n",
    "}\n",
    "\n",
    "### optimizee\n",
    "config[\"optee\"] = {\n",
    "    \"optee_cls\": MLPOptee,\n",
    "    \"optee_config\": {\n",
    "        \"layer_sizes\": [20],\n",
    "        \"act_fn\": nn.ReLU(),\n",
    "    },\n",
    "}\n",
    "\n",
    "### optimizer\n",
    "config[\"opter\"] = {\n",
    "    \"opter_cls\": L2O,\n",
    "    \"opter_config\": {\n",
    "        \"in_dim\": 3, # len(in_features) + 1\n",
    "        \"out_dim\": 3,\n",
    "        \"hidden_sz\": 40,\n",
    "        \"in_features\": (\"grad\", \"iter_num_enc\"),\n",
    "        \"base_opter_cls\": CFGD,\n",
    "        \"base_opter_config\": {\n",
    "            \"lr\": 0.05,\n",
    "            \"alpha\": None,\n",
    "            \"beta\": None,\n",
    "            \"c\": None,\n",
    "            \"s\": 1,\n",
    "            \"device\": DEVICE,\n",
    "        },\n",
    "        \"params_to_optimize\": {\n",
    "            \"alpha\": {\n",
    "                \"idx\": 0,\n",
    "                \"act_fns\": (\"sigmoid\",),\n",
    "            },\n",
    "            \"beta\": {\n",
    "                \"idx\": 1,\n",
    "                \"act_fns\": (\"identity\",),\n",
    "            },\n",
    "            \"c\": {\n",
    "                \"idx\": 2,\n",
    "                \"act_fns\": (\"identity\",),\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "config[\"meta_training_config\"] = {\n",
    "    \"meta_opter_cls\": optim.Adam,\n",
    "    \"meta_opter_config\": {\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"n_runs\": 500,\n",
    "    \"unroll\": 30,\n",
    "    \"loggers\": [\n",
    "        # {\n",
    "        #     \"every_nth_run\": 20,\n",
    "        #     \"logger_fn\": partial(plotter, to_plot=\"gamma\"),\n",
    "        # },\n",
    "        # {\n",
    "        #     \"every_nth_run\": 20,\n",
    "        #     \"logger_fn\": partial(plotter, to_plot=\"c\"),\n",
    "        # }\n",
    "    ],\n",
    "}\n",
    "\n",
    "### other\n",
    "config.update({\n",
    "    \"n_iters\": 200,\n",
    "    \"l2o_dict\": None,\n",
    "    \"additional_metrics\": {\n",
    "        # \"gamma\": lambda opter, **kwargs: \\\n",
    "        #     # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "        #     opter.base_opter.param_groups[0][\"gamma\"].mean().item() \\\n",
    "        #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"gamma\", None),\n",
    "        # \"c\": lambda opter, **kwargs: \\\n",
    "        #     # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "        #     opter.base_opter.param_groups[0][\"c\"].mean().item() \\\n",
    "        #     if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"c\", None),\n",
    "        # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "        #     torch.norm(task[\"x_tik_solution\"](gamma=1., c=1) - optee.params.detach(), p=2).item(),\n",
    "        # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "        #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "    },\n",
    "    \"ckpt_config\": {\n",
    "        \"ckpt_every_nth_run\": 20,\n",
    "        \"ckpt_dir\": os.path.join(\n",
    "            CKPT_PATH,\n",
    "            \"l2o\",\n",
    "            config[\"time\"] + \"__\"\\\n",
    "                + config[\"opter\"][\"opter_cls\"].__name__ + \"__\"\\\n",
    "                + config[\"opter\"][\"opter_config\"][\"base_opter_cls\"].__name__,\n",
    "        ),\n",
    "    },\n",
    "    \"device\": DEVICE,\n",
    "    \"seed\": 0,\n",
    "})\n",
    "config[\"ckpt_config\"][\"ckpt_dir_meta_training\"] = os.path.join(\n",
    "    config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "    \"meta_training\",\n",
    ")\n",
    "config[\"ckpt_config\"][\"ckpt_dir_meta_testing\"] = os.path.join(\n",
    "    config[\"ckpt_config\"][\"ckpt_dir\"],\n",
    "    \"meta_testing\",\n",
    ")\n",
    "\n",
    "### make dirs\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir\"], exist_ok=True)\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir_meta_training\"], exist_ok=True)\n",
    "os.makedirs(config[\"ckpt_config\"][\"ckpt_dir_meta_testing\"], exist_ok=True)\n",
    "\n",
    "### save config\n",
    "with open(os.path.join(config[\"ckpt_config\"][\"ckpt_dir\"], \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4, default=str)\n",
    "\n",
    "print(f\"Path to checkpoints: {config['ckpt_config']['ckpt_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting meta-training.\n",
      "[INFO] Meta-training starts.\n",
      "  [1/500]> sum(loss): 100.441  last(loss): 0.281\n",
      "       > new best loss sum: 100.441\n",
      "  [2/500]> sum(loss): 95.766  last(loss): 0.330\n",
      "       > new best loss sum: 95.766\n",
      "  [3/500]> sum(loss): 95.826  last(loss): 0.315\n",
      "  [4/500]> sum(loss): 95.955  last(loss): 0.279\n",
      "  [5/500]> sum(loss): 99.787  last(loss): 0.266\n",
      "  [6/500]> sum(loss): 94.588  last(loss): 0.244\n",
      "       > new best loss sum: 94.588\n",
      "  [7/500]> sum(loss): 90.156  last(loss): 0.298\n",
      "       > new best loss sum: 90.156\n",
      "  [8/500]> sum(loss): 95.510  last(loss): 0.348\n",
      "  [9/500]> sum(loss): 98.657  last(loss): 0.374\n",
      "  [10/500]> sum(loss): 97.637  last(loss): 0.278\n",
      "  [11/500]> sum(loss): 96.061  last(loss): 0.297\n",
      "  [12/500]> sum(loss): 91.102  last(loss): 0.306\n",
      "  [13/500]> sum(loss): 95.240  last(loss): 0.360\n",
      "  [14/500]> sum(loss): 95.306  last(loss): 0.343\n",
      "  [15/500]> sum(loss): 95.230  last(loss): 0.322\n",
      "  [16/500]> sum(loss): 95.405  last(loss): 0.308\n",
      "  [17/500]> sum(loss): 93.797  last(loss): 0.274\n",
      "  [18/500]> sum(loss): 95.020  last(loss): 0.235\n",
      "  [19/500]> sum(loss): 95.252  last(loss): 0.257\n",
      "  [20/500]> sum(loss): 93.312  last(loss): 0.250\n",
      "       > saving checkpoint\n",
      "  [21/500]> sum(loss): 93.893  last(loss): 0.256\n",
      "  [22/500]> sum(loss): 102.209  last(loss): 0.317\n",
      "  [23/500]> sum(loss): 92.897  last(loss): 0.305\n",
      "  [24/500]> sum(loss): 96.088  last(loss): 0.287\n",
      "  [25/500]> sum(loss): 95.772  last(loss): 0.301\n",
      "  [26/500]> sum(loss): 95.430  last(loss): 0.365\n",
      "  [27/500]> sum(loss): 100.679  last(loss): 0.310\n",
      "  [28/500]> sum(loss): 92.926  last(loss): 0.234\n",
      "  [29/500]> "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m l2o_dict, l2o_dict_best, log \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopter_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopter_cls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopter_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptee_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptee_cls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptee_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptee_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_cls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_iters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_opter_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta_training_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta_opter_cls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_opter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta_training_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta_opter_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta_training_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_runs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta_training_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munroll\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madditional_metrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta_training_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloggers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mckpt_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m### keep meta-training\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# l2o_dict=l2o_dict,\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# l2o_dict_best=l2o_dict_best,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# log=log,\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m### save checkpoint\u001b[39;00m\n\u001b[1;32m     27\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2o_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: l2o_dict,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2o_dict_best\u001b[39m\u001b[38;5;124m\"\u001b[39m: l2o_dict_best,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m\"\u001b[39m: log,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config,\n\u001b[1;32m     32\u001b[0m }, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt_config\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m), pickle_module\u001b[38;5;241m=\u001b[39mdill)\n",
      "File \u001b[0;32m~/Desktop/Dev/fl2o/fl2o/training.py:196\u001b[0m, in \u001b[0;36mmeta_train\u001b[0;34m(opter_cls, opter_config, optee_cls, optee_config, data_cls, data_config, n_iters, meta_opter_cls, meta_opter_config, n_runs, unroll, additional_metrics, loggers, l2o_dict, l2o_dict_best, log, ckpt_config)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_runs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_runs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]> \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m     _log, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdo_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopter_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptee_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptee_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptee_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptee_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml2o_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2o_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_meta_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(_log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    209\u001b[0m     log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39msum(_log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[0;32m~/Desktop/Dev/fl2o/fl2o/training.py:99\u001b[0m, in \u001b[0;36mdo_fit\u001b[0;34m(opter_cls, opter_config, optee_cls, optee_config, data_cls, data_config, n_iters, l2o_dict, in_meta_training, additional_metrics)\u001b[0m\n\u001b[1;32m     97\u001b[0m     opter\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m opter\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2o\u001b[39m\u001b[38;5;124m\"\u001b[39m,):\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mopter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptee\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptee\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miter_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     unroll_meta_loss \u001b[38;5;241m=\u001b[39m unroll_meta_loss \u001b[38;5;241m+\u001b[39m loss\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m opter\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfgd_closedform\u001b[39m\u001b[38;5;124m\"\u001b[39m,):\n",
      "File \u001b[0;32m~/Desktop/Dev/fl2o/fl2o/l2o.py:213\u001b[0m, in \u001b[0;36mL2O.step\u001b[0;34m(self, y_hat, loss, task, optee, iter_num)\u001b[0m\n\u001b[1;32m    210\u001b[0m             g[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    211\u001b[0m             g[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m p_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(p\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_opter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptee\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptee\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Dev/fl2o/fl2o/optimizer.py:355\u001b[0m, in \u001b[0;36mCFGD.step\u001b[0;34m(self, task, optee)\u001b[0m\n\u001b[1;32m    347\u001b[0m     curr_fos, curr_sos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_grads_diag_hess(\n\u001b[1;32m    348\u001b[0m         optee\u001b[38;5;241m=\u001b[39moptee_to_use,\n\u001b[1;32m    349\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m    350\u001b[0m         forward_w_params\u001b[38;5;241m=\u001b[39mpp\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;241m*\u001b[39mp\u001b[38;5;241m.\u001b[39mshape),  \u001b[38;5;66;03m# resize to (B, param_shape*) for the forward pass\u001b[39;00m\n\u001b[1;32m    351\u001b[0m         params_for\u001b[38;5;241m=\u001b[39mn,\n\u001b[1;32m    352\u001b[0m     )\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# extract only the fo and so derivatives wrt. to the changed params\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     \u001b[43mchunks_fos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurr_fos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchanged_elems\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     chunks_sos\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    359\u001b[0m         curr_sos\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mview(B, p\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mnumel())[torch\u001b[38;5;241m.\u001b[39marange(B), changed_elems]\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    361\u001b[0m fos[p_idx]\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mcat(chunks_fos)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mp\u001b[38;5;241m.\u001b[39mshape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### meta train\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "l2o_dict, l2o_dict_best, log = meta_train(\n",
    "    opter_cls=config[\"opter\"][\"opter_cls\"],\n",
    "    opter_config=config[\"opter\"][\"opter_config\"],\n",
    "    optee_cls=config[\"optee\"][\"optee_cls\"],\n",
    "    optee_config=config[\"optee\"][\"optee_config\"],\n",
    "    data_cls=config[\"data\"][\"data_cls\"],\n",
    "    data_config=config[\"data\"][\"data_config\"],\n",
    "    n_iters=config[\"n_iters\"],\n",
    "    meta_opter_cls=config[\"meta_training_config\"][\"meta_opter_cls\"],\n",
    "    meta_opter_config=config[\"meta_training_config\"][\"meta_opter_config\"],\n",
    "    n_runs=config[\"meta_training_config\"][\"n_runs\"],\n",
    "    unroll=config[\"meta_training_config\"][\"unroll\"],\n",
    "    additional_metrics=config[\"additional_metrics\"],\n",
    "    loggers=config[\"meta_training_config\"][\"loggers\"],\n",
    "    ckpt_config=config[\"ckpt_config\"],\n",
    "    \n",
    "    ### keep meta-training\n",
    "    # l2o_dict=l2o_dict,\n",
    "    # l2o_dict_best=l2o_dict_best,\n",
    "    # log=log,\n",
    ")\n",
    "\n",
    "### save checkpoint\n",
    "torch.save({\n",
    "    \"l2o_dict\": l2o_dict,\n",
    "    \"l2o_dict_best\": l2o_dict_best,\n",
    "    \"log\": log,\n",
    "    \"config\": config,\n",
    "}, os.path.join(config[\"ckpt_config\"][\"ckpt_dir\"], \"ckpt.pth\"), pickle_module=dill)\n",
    "\n",
    "plt.plot(log[\"loss_sum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta-testing config\n",
    "n_test_runs = 1\n",
    "test_run_iters = 200\n",
    "test_runs_seed = config[\"seed\"]\n",
    "\n",
    "runs = dict()\n",
    "\n",
    "# runs[\"GD\"] = {\n",
    "#     \"update_config\": {\n",
    "#         \"opter\": {\n",
    "#             \"opter_cls\": GD,\n",
    "#             \"opter_config\": {\n",
    "#                 \"lr\": 0.3,\n",
    "#                 \"device\": DEVICE,\n",
    "#             },\n",
    "#         },\n",
    "#         \"additional_metrics\": {\n",
    "#             # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "#             #     torch.norm(task[\"x_tik_solution\"](gamma=0., c=1) - optee.params.detach(), p=2).item(),\n",
    "#             # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "#             #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "#             # \"x\": lambda y_hat, y, optee: optee.params.detach().cpu().numpy(),\n",
    "#             # \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "#             #     torch.cosine_similarity(\n",
    "#             #         opter.param_groups[0][\"last_update\"].flatten(),\n",
    "#             #         opter.param_groups[0][\"last_grad\"].flatten(),\n",
    "#             #         dim=0\n",
    "#             #     ).item(),\n",
    "#             # \"last_lr\": lambda opter, **kwargs: \\\n",
    "#             #     opter.param_groups[0][\"last_lr\"].item() if type(opter.param_groups[0][\"last_lr\"]) == torch.Tensor else opter.param_groups[0][\"last_lr\"],\n",
    "#         },\n",
    "#         \"n_iters\": test_run_iters,\n",
    "#     },\n",
    "#     \"plot_config\": {\n",
    "#         \"color\": \"black\",\n",
    "#         \"linestyle\": \"dashed\",\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# for gamma in [-0.23, 0.01, 0.05, 0.5, 1.]:\n",
    "#     runs[r\"NA-CFGD_ClosedForm, $\\gamma$=\" + str(gamma)] = {\n",
    "#         \"update_config\": {\n",
    "#             \"data\": {\n",
    "#                 \"d\": test_d,\n",
    "#                 \"m\": test_m,\n",
    "#                 \"data_cls\": CustomTask,\n",
    "#                 \"data_config\": {\n",
    "#                     \"task\": generate_least_squares_task,\n",
    "#                     \"task_config\": {\n",
    "#                         \"d\": test_d,\n",
    "#                         \"m\": test_m,\n",
    "#                         \"verbose\": False,\n",
    "#                         \"device\": DEVICE,\n",
    "#                     },\n",
    "#                 },\n",
    "#             },\n",
    "#             \"optee\": {\n",
    "#                 \"optee_cls\": CustomParams,\n",
    "#                 \"optee_config\": {\n",
    "#                     \"dim\": (1, test_d),\n",
    "#                     \"init_params\": \"randn\",\n",
    "#                     # \"param_func\": None,\n",
    "#                 },\n",
    "#             },\n",
    "#             \"opter\": {\n",
    "#                 \"opter_cls\": CFGD_ClosedForm,\n",
    "#                 \"opter_config\": {\n",
    "#                     \"lr\": get_optimal_lr,\n",
    "#                     \"gamma\": gamma,\n",
    "#                     \"c\": 1.,\n",
    "#                     \"device\": DEVICE,\n",
    "#                 },\n",
    "#             },\n",
    "#             \"additional_metrics\": {\n",
    "#                 # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "#                 #     torch.norm(task[\"x_tik_solution\"](gamma=gamma, c=1) - optee.params.detach(), p=2).item(),\n",
    "#                 # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "#                 #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "#                 # \"x\": lambda y_hat, y, optee: optee.params.detach().cpu().numpy(),\n",
    "#                 \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "#                     torch.cosine_similarity(\n",
    "#                         opter.state[0][\"last_update\"].flatten(),\n",
    "#                         opter.state[0][\"last_grad\"].flatten(),\n",
    "#                         dim=0\n",
    "#                     ).item(),\n",
    "#                 \"last_lr\": lambda opter, **kwargs: \\\n",
    "#                     opter.state[0][\"last_lr\"].item() if type(opter.state[0][\"last_lr\"]) == torch.Tensor else opter.state[0][\"last_lr\"],\n",
    "#             },\n",
    "#             \"n_iters\": test_run_iters,\n",
    "#         },\n",
    "#         \"plot_config\": {\n",
    "#             \"linestyle\": \"dashed\",\n",
    "#         },\n",
    "#     }\n",
    "\n",
    "# runs[\"L2O + CFGD_ClosedForm\"] = {\n",
    "#     \"update_config\": {\n",
    "#         \"additional_metrics\": {\n",
    "#             \"gamma\": lambda opter, **kwargs: \\\n",
    "#                 # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "#                 opter.base_opter.param_groups[0][\"gamma\"].detach().cpu().numpy() \\\n",
    "#                 if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"gamma\", None),\n",
    "#             \"c\": lambda opter, **kwargs: \\\n",
    "#                 # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "#                 opter.base_opter.param_groups[0][\"c\"].detach().cpu().numpy() \\\n",
    "#                 if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"c\", None),\n",
    "#             \"alpha\": lambda opter, **kwargs: \\\n",
    "#                 # opter.base_opter.param_groups[0][\"gamma\"].item() \\\n",
    "#                 opter.base_opter.param_groups[0][\"alpha\"] \\\n",
    "#                 if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"alpha\", None),\n",
    "#             \"beta\": lambda opter, **kwargs: \\\n",
    "#                 # opter.base_opter.param_groups[0][\"c\"].item() \\\n",
    "#                 opter.base_opter.param_groups[0][\"beta\"] \\\n",
    "#                 if hasattr(opter, \"base_opter\") else opter.param_groups[0].get(\"beta\", None),\n",
    "#             # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "#             #     torch.norm(task[\"x_tik_solution\"](gamma=0., c=1) - optee.params.detach(), p=2).item(),\n",
    "#             # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "#             #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "#             # \"x\": lambda y_hat, y, optee: optee.params.detach().cpu().numpy(),\n",
    "#         },\n",
    "#         \"l2o_dict\": l2o_dict,\n",
    "#         # \"l2o_dict\": l2o_dict_best[\"best_l2o_dict\"],\n",
    "#         \"n_iters\": test_run_iters,\n",
    "#     },\n",
    "#     \"plot_config\": {\n",
    "#         \"color\": \"orange\",\n",
    "#         \"linewidth\": \"3\",\n",
    "#     },\n",
    "# }\n",
    "\n",
    "runs[\"CFGD\"] = {\n",
    "    \"update_config\": {\n",
    "        \"opter\": {\n",
    "            \"opter_cls\": CFGD,\n",
    "            \"opter_config\": {\n",
    "                \"lr\": 0.3,\n",
    "                \"alpha\": 0.9,\n",
    "                \"beta\": 0.2,\n",
    "                \"c\": 1,\n",
    "                \"s\": 1,\n",
    "                \"device\": DEVICE,\n",
    "            },\n",
    "        },\n",
    "        \"additional_metrics\": {\n",
    "            # \"l2_dist(x_tik*, x)\": lambda task, optee, **kwargs: \\\n",
    "            #     torch.norm(task[\"x_tik_solution\"](gamma=0., c=1) - optee.params.detach(), p=2).item(),\n",
    "            # \"l2_dist(x*, x)\": lambda task, optee, **kwargs: \\\n",
    "            #     torch.norm(task[\"x_solution\"] - optee.params.detach(), p=2).item(),\n",
    "            # \"x\": lambda y_hat, y, optee: optee.params.detach().cpu().numpy(),\n",
    "            # \"cos_sim(d, x.grad)\": lambda opter, **kwargs: \\\n",
    "            #     torch.cosine_similarity(\n",
    "            #         opter.state[0][\"last_update\"].flatten(),\n",
    "            #         opter.state[0][\"last_grad\"].flatten(),\n",
    "            #         dim=0\n",
    "            #     ).item(),\n",
    "            # \"last_lr\": lambda opter, **kwargs: \\\n",
    "            #     opter.state[0][\"last_lr\"].item() if type(opter.state[0][\"last_lr\"]) == torch.Tensor else opter.state[0][\"last_lr\"],\n",
    "        },\n",
    "        \"n_iters\": test_run_iters,\n",
    "    },\n",
    "    \"plot_config\": {\n",
    "        \"color\": \"orange\",\n",
    "        \"linewidth\": \"3\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run all\n",
    "for run_name in runs.keys():\n",
    "    if \"log\" in runs[run_name]:\n",
    "        continue # already run\n",
    "    run_config = copy.deepcopy(config)\n",
    "    if \"update_config\" in runs[run_name] and runs[run_name][\"update_config\"] is not None:\n",
    "        run_config.update(runs[run_name][\"update_config\"])\n",
    "    print(f\"{run_name}:\")\n",
    "    \n",
    "    torch.manual_seed(test_runs_seed)\n",
    "    np.random.seed(test_runs_seed)\n",
    "\n",
    "    if \"lr\" in run_config[\"opter\"][\"opter_config\"] \\\n",
    "        and run_config[\"opter\"][\"opter_config\"][\"lr\"] == find_best_lr:\n",
    "        print(\"  > Finding best lr...\")\n",
    "        run_config[\"opter\"][\"opter_config\"][\"lr\"] = find_best_lr(\n",
    "            opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "            opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "            optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "            optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "            data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "            data_config=run_config[\"data\"][\"data_config\"],\n",
    "            # loss_fn=run_config[\"loss_fn\"],\n",
    "            n_iters=120,\n",
    "            n_tests=1,\n",
    "            consider_metric=\"loss\",\n",
    "            lrs_to_try=[0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0],\n",
    "        )\n",
    "        print(f\"  > Best lr: {run_config['opter']['opter_config']['lr']}\")\n",
    "\n",
    "    print(\"  > Running...\")\n",
    "    runs[run_name][\"log\"] = dict()\n",
    "    for i in range(n_test_runs):\n",
    "        print(f\"    > Run {i+1}/{n_test_runs}...\")\n",
    "\n",
    "        ### check if L2O has been meta-trained\n",
    "        assert not run_config[\"opter\"][\"opter_cls\"] == L2O or run_config[\"l2o_dict\"] is not None\n",
    "\n",
    "        curr_log = do_fit(\n",
    "            opter_cls=run_config[\"opter\"][\"opter_cls\"],\n",
    "            opter_config=run_config[\"opter\"][\"opter_config\"],\n",
    "            optee_cls=run_config[\"optee\"][\"optee_cls\"],\n",
    "            optee_config=run_config[\"optee\"][\"optee_config\"],\n",
    "            data_cls=run_config[\"data\"][\"data_cls\"],\n",
    "            data_config=run_config[\"data\"][\"data_config\"],\n",
    "            n_iters=run_config[\"n_iters\"],\n",
    "            l2o_dict=run_config[\"l2o_dict\"],\n",
    "            in_meta_training=False,\n",
    "            additional_metrics=run_config[\"additional_metrics\"],\n",
    "        )[0]\n",
    "\n",
    "        for metric_name in curr_log.keys():\n",
    "            if metric_name not in runs[run_name][\"log\"]:\n",
    "                runs[run_name][\"log\"][metric_name] = []\n",
    "            runs[run_name][\"log\"][metric_name].append(curr_log[metric_name])\n",
    "\n",
    "    runs[run_name][\"config\"] = run_config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
